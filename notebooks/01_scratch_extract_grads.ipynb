{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets save our data as a huggingface dataset, so it's quick to reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:39.840442Z",
     "start_time": "2023-09-02T11:00:38.221653Z"
    }
   },
   "outputs": [],
   "source": [
    "# import your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from loguru import logger\n",
    "import sys\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"<level>{message}</level>\", level=\"INFO\")\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:42.996618Z",
     "start_time": "2023-09-02T11:00:39.841585Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetInfo, load_from_disk, load_dataset\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, re, sys, collections, functools, itertools, json\n",
    "\n",
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.258472Z",
     "start_time": "2023-09-02T11:00:43.000477Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models.load import load_model\n",
    "from src.datasets.load import ds2df\n",
    "from src.datasets.load import rows_item\n",
    "from src.datasets.batch import batch_hidden_states\n",
    "# from src.datasets.scores import choice2ids, scores2choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.316850Z",
     "start_time": "2023-09-02T11:00:46.259480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "BATCH_SIZE = 1  # None # None means auto # 6 gives 16Gb/25GB. where 10GB is the base model. so 6 is 6/15\n",
    "USE_MCDROPOUT = True\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    # model=\"HuggingFaceH4/starchat-beta\",\n",
    "    # model=\"TheBloke/CodeLlama-13B-Instruct-fp16\", # too large!\n",
    "    model=\"WizardLM/WizardCoder-3B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-1B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-Python-7B-V1.0\", # too large!\n",
    "    datasets = [\n",
    "        \"imdb\", \n",
    "                ],\n",
    "    max_examples=(400, 312),\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Chosing:\n",
    "- https://old.reddit.com/r/LocalLLaMA/wiki/models\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- https://github.com/deep-diver/LLM-As-Chatbot/blob/main/model_cards.json\n",
    "\n",
    "\n",
    "A uncensored and large coding ones might be best for lying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:50.889443Z",
     "start_time": "2023-09-02T11:00:46.318029Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models.load import verbose_change_param, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_repo = \"HuggingFaceH4/starchat-beta\"):\n",
    "    # see https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/starchat.py\n",
    "    model_options = dict(\n",
    "        device_map=\"auto\",\n",
    "        # load_in_8bit=True,\n",
    "        # load_in_4bit=True,\n",
    "        torch_dtype=torch.float16, # note because datasets pickles the model into numpy to get the unique datasets name, and because numpy doesn't support bfloat16, we need to use float16\n",
    "        # use_safetensors=False,\n",
    "    )\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_repo, use_cache=False)\n",
    "    verbose_change_param(config, 'use_cache', False)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "    verbose_change_param(tokenizer, 'pad_token_id', 0)\n",
    "    verbose_change_param(tokenizer, 'padding_side', 'left')\n",
    "    verbose_change_param(tokenizer, 'truncation_side', 'left')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_repo, config=config, **model_options)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_y = tokenizer(' True').input_ids\n",
    "token_n = tokenizer(' Fakse').input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525457Z",
     "start_time": "2023-09-02T11:02:54.525448Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from itertools import chain, islice\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "# from datasets.arrow_dataset import Dataset\n",
    "from src.prompts.prompt_loading import load_prompts\n",
    "\n",
    "@functools.lru_cache()\n",
    "def count_tokens(s):\n",
    "    return len(tokenizer(s).input_ids)\n",
    "\n",
    "def answer_len(answer_choices: list):\n",
    "    a = count_tokens(answer_choices[0])\n",
    "    b = count_tokens(answer_choices[1])\n",
    "    return max(a, b)\n",
    "\n",
    "\n",
    "def sample_n_true_y_false_prompts(prompts, num_truth=1, num_lie=1, seed=42):\n",
    "    \"\"\"sample some truth and some false\"\"\"\n",
    "    df = pd.DataFrame(prompts)\n",
    "    \n",
    "    # restrict to template where the choices are a single token\n",
    "    m = df.answer_choices.map(answer_len)<=2\n",
    "    df = df[m]\n",
    "    df = pd.concat([\n",
    "        df.query(\"instructed_to_lie==True\").sample(num_truth, random_state=seed),\n",
    "        df.query(\"instructed_to_lie==False\").sample(num_lie, random_state=seed)])\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "        \n",
    "# loop through all prompts in this dataset\n",
    "ds_names = cfg.datasets\n",
    "split_type = \"train\"\n",
    "\n",
    "ds_name = ds_names[0]\n",
    "prompt_ds = load_prompts(\n",
    "    ds_name,\n",
    "    num_shots=cfg.num_shots,\n",
    "    split_type=split_type,\n",
    "    template_path=cfg.template_path,\n",
    "    seed=cfg.seed,\n",
    "    prompt_format='llama'\n",
    ")\n",
    "\n",
    "# for each example, sample true and false\n",
    "N = cfg.max_examples[split_type!=\"train\"]\n",
    "g = map(lambda r: sample_n_true_y_false_prompts(r[1], seed=r[0]+cfg.seed), enumerate(prompt_ds))\n",
    "\n",
    "# and combine them into one big list\n",
    "g = chain.from_iterable(g) \n",
    "prompt_ds2 = list(tqdm(islice(g, N), total=N))\n",
    "\n",
    "# convert to hugginface dataset\n",
    "dataset = Dataset.from_list(prompt_ds2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525970Z",
     "start_time": "2023-09-02T11:02:54.525961Z"
    }
   },
   "outputs": [],
   "source": [
    "b = next(iter(prompt_ds))\n",
    "b\n",
    "sample_n_true_y_false_prompts(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format prompts\n",
    "\n",
    "The prompt is the thing we most often have to change and debug. So we do it explicitly here.\n",
    "\n",
    "We do it as transforms on a huggingface dataset.\n",
    "\n",
    "In this case we use multishot examples from train, and use the test set to generated the hidden states dataset. We will test generalisation on a whole new dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.scores import scores2choice_probs\n",
    "from src.datasets.scores import choice2id, choice2ids\n",
    "\n",
    "def row_choice_ids(r):\n",
    "    return choice2ids([[c] for c in r['answer_choices']], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.526826Z",
     "start_time": "2023-09-02T11:02:54.526815Z"
    },
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = (\n",
    "    dataset\n",
    "    .map(\n",
    "        lambda ex: tokenizer(\n",
    "            ex[\"question\"], padding=\"max_length\", max_length=600, truncation=True, add_special_tokens=True,\n",
    "            # return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(\n",
    "        lambda r: {\"prompt_truncated\": tokenizer.batch_decode(r[\"input_ids\"])},\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(lambda r: {'choice_ids': row_choice_ids(r)})\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[0].keys()\n",
    "\n",
    "torch_cols = ['input_ids', 'attention_mask', 'choice_ids']\n",
    "\n",
    "ds_o = ds.remove_columns(torch_cols)\n",
    "ds.set_format('torch', torch_cols)\n",
    "row = ds[0]\n",
    "row_0 = ds_o[0]\n",
    "row.keys()\n",
    "# prompt =row['question']\n",
    "# prompt\n",
    "# row.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_0.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, choice_ids = row['input_ids'].to(model.device)[None, :], row['attention_mask'].to(model.device)[None, :], row['choice_ids'].to(model.device)[None, :]\n",
    "choice_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note bigcode vs normal llamba. one has self attention one has cross\n",
    "- [llama2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\n",
    "- [gpt_bigcode](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py)\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "- [honest_llama](https://github.com/likenneth/honest_llama/blob/e010f82bfbeaa4326cef8493b0dd5b8b14c6da67/utils.py#L159)\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "- [tracedict](https://github.com/davidbau/baukit/blob/main/baukit/nethook.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(model, scores, token_y, token_n):\n",
    "    model.zero_grad()\n",
    "    assert token_y.shape[1]<2, 'FIXME just use the first token for now'\n",
    "    score_y = torch.index_select(scores, 1, token_y[:, 0])\n",
    "    score_n = torch.index_select(scores, 1, token_n[:, 0])\n",
    "    pred = score_y - score_n\n",
    "    loss = F.mse_loss(pred, -pred)\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "output = scores = None\n",
    "model.eval()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import Trace, TraceDict\n",
    "HEADS = [f\"transformer.h.{i}.attn.c_proj\" for i in range(model.config.num_hidden_layers)]\n",
    "MLPS = [f\"transformer.h.{i}.mlp\" for i in range(model.config.num_hidden_layers)]\n",
    "model.train()\n",
    "with torch.autocast('cuda' dtype=torch.bfloat16):\n",
    "    with TraceDict(model, HEADS+MLPS, retain_grad=True) as ret:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        scores = outputs.logits[:, -1, :]\n",
    "        \n",
    "        token1_n = choice_ids[:, 0] # [batch, tokens]\n",
    "        token1_y = choice_ids[:, 1]\n",
    "        get_gradients(model, scores, token1_y, token1_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(HEADS), len(MLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_wise_hidden_states = [ret[head].output.squeeze().detach().cpu() for head in HEADS]\n",
    "# torch.stack(head_wise_hidden_states, dim=0)[:, -1].squeeze().numpy().shape\n",
    "def stack_trace_returns(ret: TraceDict, HEADS: List[str]) -> torch.Tensor:\n",
    "    hs = [ret[head].output.squeeze().detach().cpu() for head in HEADS]\n",
    "    return torch.stack(hs, dim=0).squeeze().numpy()[:, -1]\n",
    "\n",
    "hidden_states = torch.stack(outputs.hidden_states, dim=0).squeeze()\n",
    "hidden_states = hidden_states.detach().cpu().numpy()[:, -1]\n",
    "\n",
    "head_wise_hidden_states = stack_trace_returns(ret, HEADS)\n",
    "mlp_wise_hidden_states = stack_trace_returns(ret, MLPS)\n",
    "hidden_states.shape, head_wise_hidden_states.shape, mlp_wise_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ret['transformer.h.0.attn.c_proj']\n",
    "a.output.grad.shape, a.output.shape\n",
    "# dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ret['transformer.h.0.mlp']\n",
    "a.output.grad.shape, a.output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
