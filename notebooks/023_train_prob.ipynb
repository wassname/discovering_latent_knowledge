{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distance and direciton\n",
    "\n",
    "Let try to opt for distance and direction with\n",
    "\n",
    "$L1loss(y_1-y_0, y_{true})$\n",
    "\n",
    "where $y_1=model(x_1)$\n",
    "\n",
    "So I'm optimising for the hidden states to be the correct distance and direcioton away. It's like the margin raning loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "links:\n",
    "- [loading](https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/alpaca.py)\n",
    "- [dict](https://github.com/deep-diver/LLM-As-Chatbot/blob/c79e855a492a968b54bac223e66dc9db448d6eba/model_cards.json#L143)\n",
    "- [prompt_format](https://github.com/deep-diver/PingPong/blob/main/src/pingpong/alpaca.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "fs = [\n",
    "    '../.ds/model-starchat-beta_ds-amazon-polarity_format-guard-prompt_N8000_3shots_07e51a'\n",
    "    # '../.ds/model-starchat-beta_ds-amazon-polarity_format-guard-prompt_N2000_2shots_6df747'\n",
    "]\n",
    "\n",
    "# './.ds/HuggingFaceH4starchat_beta-None-N_8000-ns_3-mc_0.2-2ffc1e'\n",
    "ds1 = concatenate_datasets([load_from_disk(f) for f in fs])\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.load import ds2df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets select only the ones where\n",
    "df = ds2df(ds1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just select the question where the model knows the answer. \n",
    "# d = df.query('version==\"truth\"').set_index(\"index\")\n",
    "# # these are the ones where it got it right when asked to tell the truth\n",
    "# known_indices = d[d.llm_ans==d.true_answer].index\n",
    "\n",
    "# # convert to row numbers, and use datasets to select\n",
    "# known_rows = df['index'].isin(known_indices)\n",
    "# known_rows_i = df[known_rows].index\n",
    "\n",
    "# also restrict it to significant permutations. That is monte carlo dropout pairs, where the answer changes by more than X%\n",
    "m = np.abs(df.ans0-df.ans1)>0.1\n",
    "significant_rows = m[m].index\n",
    "\n",
    "# allowed_rows_i = set(known_rows_i).intersection(significant_rows)\n",
    "allowed_rows_i = significant_rows\n",
    "ds = ds1.select(allowed_rows_i)\n",
    "print(f\"selected rows are {len(ds)/len(ds1):2.2%}\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform: Normalize by activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "# small_ds = ds.select(range(N))\n",
    "# b = N\n",
    "# hs0 = small_ds['hs0'].reshape((b, -1))\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "# hs1 = scaler.fit_transform(hs0)\n",
    "\n",
    "# def normalize_hs(hs0, hs1):\n",
    "#     shape=hs0.shape\n",
    "#     b = len(hs0)\n",
    "#     hs0 = scaler.transform(hs0.reshape((b, -1))).reshape(shape)\n",
    "#     hs1 = scaler.transform(hs1.reshape((b, -1))).reshape(shape)\n",
    "#     return {'hs0':hs0, 'hs1': hs1}\n",
    "\n",
    "# # Plot\n",
    "# plt.hist(hs0.flatten(), bins=155, range=[-5, 5], label='before', histtype='step')\n",
    "# plt.hist(hs1.flatten(), bins=155, range=[-5, 5], label='after', histtype='step')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # # Test\n",
    "# # small_dataset = ds.select(range(4))\n",
    "# # small_dataset.map(normalize_hs, batched=True, batch_size=2, input_columns=['hs0', 'hs1'])\n",
    "\n",
    "# # run\n",
    "# ds = ds.map(normalize_hs, batched=True, input_columns=['hs0', 'hs1'])\n",
    "# ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds2df(ds)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we detecting? If the right example of the pair is more deceptive.\n",
    "\n",
    "Now it's only deceptive if\n",
    "- it was asked to lie\n",
    "- it knows the truth\n",
    "- it gave the wrong answer (around 10% of the time)( it's hard to get these models to lie by encouragement rather than instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers import switch2bool, bool2switch\n",
    "from src.datasets.dm import imdbHSDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 120\n",
    "# test and cache\n",
    "dm = imdbHSDataModule(ds, batch_size=batch_size)\n",
    "dm.setup('train')\n",
    "\n",
    "dl_val = dm.val_dataloader()\n",
    "dl_train = dm.train_dataloader()\n",
    "len(dl_train), len(dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl_train))\n",
    "x0, x1, y = b\n",
    "x0.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep\n",
    "\n",
    "We do two inferences on the same inputs. Since we have dropout enabled, even during inference, we get two slightly different hidden states `hs1` and `hs2`, and two slightly different probabilities for our yes and no output tokens `p1` `p2`. We also have the true answer `t`\n",
    "\n",
    "So there are a few ways we can set up the problem. \n",
    "\n",
    "We can vary x:\n",
    "- `model(hs1)-model(hs2)=y`\n",
    "- `model(hs1-hs2)==y`\n",
    "\n",
    "And we can try differen't y's:\n",
    "- direction with a ranked loss. This could be unsupervised.\n",
    "- magnitude with a regression loss\n",
    "- vector (direction and magnitude) with a regression loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC: Linear supervised probes\n",
    "\n",
    "\n",
    "Let's verify that the model's representations are good\n",
    "\n",
    "Before trying CCS, let's make sure there exists a direction that classifies examples as true vs false with high accuracy; if supervised logistic regression accuracy is bad, there's no hope of unsupervised CCS doing well.\n",
    "\n",
    "Note that because logistic regression is supervised we expect it to do better but to have worse generalisation that equivilent unsupervised methods. However in this case CSS is using a deeper model so it is more complicated.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a classification of direction to truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "\n",
    "# Define X and y\n",
    "X = (dm.hs1-dm.hs0).reshape((n, -1))#/dm.y[:, None]\n",
    "y = dm.y>0\n",
    "\n",
    "# split\n",
    "n = len(y)\n",
    "max_rows = 300\n",
    "print('split size', n//2)\n",
    "X_train, X_test = X[:n//2], X[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "X_train = X_train[:max_rows]\n",
    "y_train = y_train[:max_rows]\n",
    "X_test = X_test[:max_rows]\n",
    "y_test = y_test[:max_rows]\n",
    "\n",
    "# scale\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train2 = scaler.transform(X_train)\n",
    "X_test2 = scaler.transform(X_test)\n",
    "print('lr')\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\", penalty=\"l2\", max_iter=100)\n",
    "lr.fit(X_train2, y_train>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic cls acc: {:2.2%} [TRAIN]\".format(lr.score(X_train2, y_train>0)))\n",
    "print(\"Logistic cls acc: {:2.2%} [TEST]\".format(lr.score(X_test2, y_test>0)))\n",
    "\n",
    "m = df['lie'][n//2:][:max_rows]\n",
    "y_test_pred = lr.predict(X_test2)\n",
    "acc_w_lie = ((y_test_pred[m]>0)==(y_test[m]>0)).mean()\n",
    "acc_wo_lie = ((y_test_pred[~m]>0)==(y_test[~m]>0)).mean()\n",
    "print(f'test acc w lie {acc_w_lie:2.2%}')\n",
    "print(f'test acc wo lie {acc_wo_lie:2.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_baseline = roc_auc_score(y_test>0, y_test_pred)\n",
    "# primary_baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.conv import PLConvProbe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiet please\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*F-score.*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataloader/set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "b = next(iter(dl_train))\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the model\n",
    "max_epochs = 42\n",
    "c_in = b[0].shape[1]\n",
    "print(b[0].shape)\n",
    "net = PLConvProbe(c_in=c_in, total_steps=max_epochs*len(dl_train), depth=5, hs=42, lr=3e-3, \n",
    "        #   weight_decay=1e-4, \n",
    "          # dropout=0.2,\n",
    "          )\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "with torch.no_grad():\n",
    "    b = next(iter(dl_train))\n",
    "    b2 = [bb.to(net.device) for bb in b]\n",
    "    y = net(b2[0])\n",
    "y.shape, b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG\n",
    "# trainer = pl.Trainer(fast_dev_run=2)\n",
    "# trainer.fit(model=net, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0611e9a170c1439383d4061e812ff420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d1074681ad4a22a739a58a2d8f43da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e664be49b39e4349b4c4bad96202cb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3039ddc7f140455bb490d88ee34cfdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d11cdbc224d758f38bec7f378807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(precision=\"bf16-mixed\",\n",
    "                     \n",
    "                     gradient_clip_val=20,\n",
    "                     max_epochs=max_epochs, log_every_n_steps=5)\n",
    "trainer.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path).ffill().bfill()\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['loss']:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot(logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['acc']:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = dm.test_dataloader()\n",
    "rs = trainer.test(net, dataloaders=[dl_train, dl_val, dl_test])\n",
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = dm.test_dataloader()\n",
    "r = trainer.predict(net, dataloaders=dl_test)\n",
    "y_test_pred = np.concatenate(r)\n",
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dm.df.iloc[dm.splits['test'][0]:].copy()\n",
    "y_true = dl_test.dataset.tensors[2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction dataframe with everything in it\n",
    "df_test = dm.df.iloc[dm.splits['test'][0]:].copy()\n",
    "df_test['probe_pred'] = y_test_pred>0\n",
    "y_test_pred_bool = np.clip(switch2bool(y_test_pred), 0 ,1)\n",
    "df_test['probe_prob'] = y_test_pred_bool\n",
    "df_test['llm_prob'] = (df_test['ans0']+df_test['ans1'])/2\n",
    "df_test['llm_ans'] = df_test['llm_prob']>0.5\n",
    "df_test['conf'] = (df_test['ans0']-df_test['ans1']).abs()\n",
    "df_test['y'] = df_test['y']>0\n",
    "\n",
    "y_true = dl_test.dataset.tensors[2].numpy()\n",
    "assert ((df_test['y'].values>0.5)==(y_true>0)).all(), 'check it all lines up'\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_subset(df, query):\n",
    "    df_s = df.query(query)\n",
    "    acc = (df_s['probe_pred']==df_s['y']).mean()\n",
    "    print(f\"acc={acc:2.2%} [{query}]\")\n",
    "    return acc\n",
    "    \n",
    "print('probe results on subsets of the data')\n",
    "get_acc_subset(df_test, 'lie==True') # it was ph told to lie\n",
    "get_acc_subset(df_test, 'lie==False') # it was told not to lie\n",
    "get_acc_subset(df_test, 'llm_ans==label') # the llm gave the true ans\n",
    "get_acc_subset(df_test, 'llm_ans==desired_ans') # the llm gave the desired ans\n",
    "get_acc_subset(df_test, 'lie==True & llm_ans==desired_ans') # it was told to lie, and it did lie\n",
    "get_acc_subset(df_test, 'lie==True & llm_ans!=desired_ans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (df_test['y']==(y_test_pred_bool>0.5)).mean()\n",
    "\n",
    "# print(f\"  PRIMARY BASELINE roc_auc={primary_baseline:2.2%} from linear classifier\")\n",
    "print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
