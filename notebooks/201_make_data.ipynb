{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pipelines as this https://github.com/wassname/representation-engineering/blob/random_comments_ignore/examples/honesty/honesty.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union, Tuple, Callable, Iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from src.repe import repe_pipeline_registry\n",
    "repe_pipeline_registry()\n",
    "\n",
    "from src.models.load import load_model\n",
    "from src.extraction.config import ExtractConfig\n",
    "from make_dataset import create_hs_ds, load_preproc_dataset\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "# from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='TheBloke/WizardCoder-Python-13B-V1.0-GPTQ', data_dirs=(), max_examples=(100, 100), num_shots=1, num_variants=-1, layers=(), seed=42, template_path=None, max_length=555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 10:49:20.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mchanging pad_token_id from 32000 to 0\u001b[0m\n",
      "2023-10-24T10:49:20.938690+0800 INFO changing pad_token_id from 32000 to 0\n",
      "\u001b[32m2023-10-24 10:49:20.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mchanging padding_side from right to left\u001b[0m\n",
      "2023-10-24T10:49:20.940322+0800 INFO changing padding_side from right to left\n",
      "\u001b[32m2023-10-24 10:49:20.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mchanging truncation_side from right to left\u001b[0m\n",
      "2023-10-24T10:49:20.941383+0800 INFO changing truncation_side from right to left\n"
     ]
    }
   ],
   "source": [
    "# model_name_or_path = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "# model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "model_name_or_path = \"TheBloke/WizardCoder-Python-13B-V1.0-GPTQ\"\n",
    "\n",
    "cfg = ExtractConfig(max_examples=(100, 100), model=model_name_or_path)\n",
    "print(cfg)\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_token = -1\n",
    "batch_size = 2\n",
    "# hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "# hidden_layers = [f\"model.layers.{i}\" for i in range(8, model.config.num_hidden_layers, 3)]\n",
    "hidden_layers = list(range(8, model.config.num_hidden_layers, 3))\n",
    "hidden_layers           \n",
    "\n",
    "n_difference = 1\n",
    "direction_method = 'pca'\n",
    "rep_reading_pipeline =  pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)\n",
    "rep_reading_pipeline\n",
    "hidden_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize: 100%|██████████| 302/302 [00:00<00:00, 3490.12 examples/s]\n",
      "truncated: 100%|██████████| 302/302 [00:00<00:00, 3889.90 examples/s]\n",
      "prompt_truncated: 100%|██████████| 302/302 [00:00<00:00, 604.08 examples/s]\n",
      "choice_ids: 100%|██████████| 302/302 [00:00<00:00, 9749.77 examples/s]\n",
      "Filter: 100%|██████████| 302/302 [00:00<00:00, 3987.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed truncated rows to leave: num_rows 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'question', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'input_ids', 'attention_mask', 'truncated', 'prompt_truncated', 'choice_ids'],\n",
       "    num_rows: 97\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds_name = 'imdb'\n",
    "ds_tokens = load_preproc_dataset(ds_name, cfg, tokenizer)\n",
    "ds_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'question', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'input_ids', 'attention_mask', 'truncated', 'prompt_truncated', 'choice_ids'],\n",
       "    num_rows: 54\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_fit_examples = 10\n",
    "N_train_split = (len(ds_tokens) - N_fit_examples) //2\n",
    "\n",
    "# split the dataset, it's preshuffled\n",
    "dataset_fit = ds_tokens.select(range(N_fit_examples))\n",
    "dataset_train = ds_tokens.select(range(N_fit_examples, N_train_split))\n",
    "dataset_test = ds_tokens.select(range(N_train_split, len(ds_tokens)))\n",
    "dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_args=dict(padding=\"max_length\", max_length=cfg.max_length, truncation=True, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.repe.rep_readers.PCARepReader at 0x7fcd97b27250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "train_labels = dataset_fit['label_true']\n",
    "honesty_rep_reader = rep_reading_pipeline.get_directions(\n",
    "    dataset_fit['question'], \n",
    "    rep_token=rep_token, \n",
    "    hidden_layers=hidden_layers, \n",
    "    n_difference=n_difference, \n",
    "    train_labels=dataset_fit['label_true'], \n",
    "    direction_method=direction_method,\n",
    "    batch_size=batch_size,\n",
    "    **tokenizer_args\n",
    ")\n",
    "honesty_rep_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: -7.1495447,\n",
       " 11: 16.796896,\n",
       " 14: 13.5725565,\n",
       " 17: -5.684828,\n",
       " 20: 26.571663,\n",
       " 23: 37.01544,\n",
       " 26: -67.76076,\n",
       " 29: -64.502205,\n",
       " 32: -52.72602,\n",
       " 35: -76.70331,\n",
       " 38: -80.74953}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read direction for each example, layer\n",
    "H_tests = rep_reading_pipeline(\n",
    "    dataset_train['question'], \n",
    "    rep_token=rep_token, \n",
    "    hidden_layers=hidden_layers, \n",
    "    rep_reader=honesty_rep_reader,\n",
    "    batch_size=batch_size, **tokenizer_args)\n",
    "H_tests[0] # {Batch, layers}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# layer_id = hidden_layers\n",
    "# block_name=\"decoder_block\"\n",
    "# control_method=\"reading_vec\"\n",
    "\n",
    "# rep_control_pipeline = pipeline(\n",
    "#     \"rep-control\", \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     layers=layer_id, max_length=cfg.max_length,\n",
    "#     control_method=control_method)\n",
    "# rep_control_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from re import S\n",
    "\n",
    "\n",
    "# inputs = dataset_train[:2]\n",
    "# coeff=8.0\n",
    "# max_new_tokens=1\n",
    "# text_gen_kwargs = dict(do_sample=False, max_new_tokens=max_new_tokens, use_cache=False, \n",
    "#                        output_hidden_states=True, return_dict=True,\n",
    "#                        )\n",
    "\n",
    "# activations = {}\n",
    "# for layer in layer_id:\n",
    "#     activations[layer] = torch.tensor(coeff * honesty_rep_reader.directions[layer] * honesty_rep_reader.direction_signs[layer]).to(model.device).half()\n",
    "    \n",
    "\n",
    "# activations_neg = {k:-v for k,v in activations.items()}\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     baseline_outputs = rep_control_pipeline(inputs, batch_size=batch_size, **text_gen_kwargs)\n",
    "#     control_outputs = rep_control_pipeline(inputs, activations=activations, batch_size=batch_size,  **text_gen_kwargs)\n",
    "#     control_outputs_neg = rep_control_pipeline(inputs, activations=activations_neg, batch_size=batch_size, **text_gen_kwargs)\n",
    "\n",
    "# for i,s,p,n in zip(inputs, baseline_outputs['text_ans'], control_outputs['text_ans'], control_outputs_neg['text_ans']):\n",
    "#     print(\"===== No Control =====\")\n",
    "#     print(s)\n",
    "#     print(f\"===== + Honesty Control =====\")\n",
    "#     print(p)\n",
    "#     print()\n",
    "#     print(f\"===== - Honesty Control =====\")\n",
    "#     print(n)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# control v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.repe.rep_control_pipeline_baukit.RepControlPipeline2 at 0x7fcca5807130>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_id = hidden_layers\n",
    "\n",
    "rep_control_pipeline = pipeline(\n",
    "    \"rep-control2\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    layers=layer_id, \n",
    "    max_length=cfg.max_length,)\n",
    "rep_control_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== No Control =====\n",
      "negative\n",
      "===== + Honesty Control =====\n",
      "negative\n",
      "\n",
      "===== - Honesty Control =====\n",
      "negative\n",
      "\n",
      "===== No Control =====\n",
      "The\n",
      "===== + Honesty Control =====\n",
      "The\n",
      "\n",
      "===== - Honesty Control =====\n",
      "negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = dataset_train[:2]\n",
    "coeff=8.0\n",
    "max_new_tokens=3\n",
    "text_gen_kwargs = dict(do_sample=False, max_new_tokens=max_new_tokens, use_cache=False, \n",
    "                       output_hidden_states=True, return_dict=True, max_length=cfg.max_length,\n",
    "                       )\n",
    "\n",
    "activations = {}\n",
    "for layer in layer_id:\n",
    "    activations[layer] = torch.tensor(coeff * honesty_rep_reader.directions[layer] * honesty_rep_reader.direction_signs[layer]).to(model.device).half()\n",
    "    \n",
    "\n",
    "activations_neg = {k:-v for k,v in activations.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = rep_control_pipeline(inputs, batch_size=batch_size, **text_gen_kwargs)\n",
    "    control_outputs = rep_control_pipeline(inputs, activations=activations, batch_size=batch_size,  **text_gen_kwargs)\n",
    "    control_outputs_neg = rep_control_pipeline(inputs, activations=activations_neg, batch_size=batch_size, **text_gen_kwargs)\n",
    "\n",
    "\n",
    "for i,s,p,n in zip(inputs, baseline_outputs['text_ans'], control_outputs['text_ans'], control_outputs_neg['text_ans']):\n",
    "    print(\"===== No Control =====\")\n",
    "    print(s)\n",
    "    print(f\"===== + Honesty Control =====\")\n",
    "    print(p)\n",
    "    print()\n",
    "    print(f\"===== - Honesty Control =====\")\n",
    "    print(n)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== No Control =====\n",
      "2.03%\n",
      "===== + Honesty Control =====\n",
      "23.10%\n",
      "\n",
      "===== - Honesty Control =====\n",
      "1.02%\n",
      "\n",
      "===== No Control =====\n",
      "33.63%\n",
      "===== + Honesty Control =====\n",
      "83.52%\n",
      "\n",
      "===== - Honesty Control =====\n",
      "7.75%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,s,p,n in zip(inputs, baseline_outputs['ans'], control_outputs['ans'], control_outputs_neg['ans']):\n",
    "    print(\"===== No Control =====\")\n",
    "    print(f'{s:02.2%}')\n",
    "    print(f\"===== + Honesty Control =====\")\n",
    "    print(f'{p:02.2%}')\n",
    "    print()\n",
    "    print(f\"===== - Honesty Control =====\")\n",
    "    print(f'{n:02.2%}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
