{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A scratch pad to run model inference manually\n",
    "\n",
    "Prioritise small experiments in notebooks\n",
    "- take all the recorded hidden states and seperate into truth and deception\n",
    "- try a normal intervention and test it\n",
    "- try novel sgb bias intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n",
    "from src.datasets.intervene import create_cache_interventions\n",
    "from src.prompts.prompt_loading import load_prompt_structure\n",
    "from src.repe import repe_pipeline_registry\n",
    "\n",
    "repe_pipeline_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config transformers\n",
    "# from datasets import set_caching_enabled, disable_caching\n",
    "# disable_caching()\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"phi-2-GPTQ_w_hidden_states\"\n",
    "[str(s) for s in sorted(Path(\"../.ds/\").glob(f\"*{model_name}*\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from src.datasets.load import ds2df, load_ds, get_ds_name\n",
    "from src.datasets.load import ds2df, load_ds, get_ds_name, filter_ds_to_known\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs = [\n",
    "    \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_amazon_polarity_test_600\",\n",
    "    # \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_amazon_polarity_train_3600\",\n",
    "    \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_glue_qnli_test_600\",\n",
    "    # \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_glue_qnli_train_3600\",\n",
    "    \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_imdb_test_600\",\n",
    "    # \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_imdb_train_3600\",\n",
    "    \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_super_glue_boolq_test_600\",\n",
    "    # \"../.ds/wassname_phi-2-GPTQ_w_hidden_states_super_glue_boolq_train_3600\",\n",
    "]\n",
    "dss = [load_ds(f) for f in fs]\n",
    "dss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.load import ds2df, load_ds, get_ds_name, filter_ds_to_known, qc_ds\n",
    "for ds in dss:\n",
    "    qc_ds(ds)\n",
    "    # ds = ds.with_format(\"numpy\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 93.00% based on knowledge\n",
      "select rows are 61.33% based on knowledge\n",
      "select rows are 84.14% based on knowledge\n",
      "select rows are 82.00% based on knowledge\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['end_hidden_states', 'end_logits', 'choice_probs', 'label_true', 'instructed_to_lie', 'question', 'answer_choices', 'choice_ids', 'template_name', 'sys_instr_name', 'example_i', 'input_truncated', 'truncated', 'text_ans', 'ans'],\n",
       "    num_rows: 1870\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine\n",
    "dss_known = [filter_ds_to_known(d) for d in dss]\n",
    "ds = concatenate_datasets(dss_known)\n",
    "ds = ds.with_format(\"numpy\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering we have 137 num successful lies out of 1870 dataset rows\n"
     ]
    }
   ],
   "source": [
    "# QC: make sure we didn't lose all of the successful lies, which would make the problem trivial\n",
    "df2 = ds2df(ds)\n",
    "df_subset_successull_lies = df2.query(\n",
    "    \"instructed_to_lie==True & ((llm_ans==1)==label_instructed)\"\n",
    ")\n",
    "print(\n",
    "    f\"after filtering we have {len(df_subset_successull_lies)} num successful lies out of {len(df2)} dataset rows\"\n",
    ")\n",
    "assert (\n",
    "    len(df_subset_successull_lies) > 0\n",
    "), \"there should be successful lies in the dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"amazon_polarity\"\n",
    "cfg = ExtractConfig(\n",
    "    max_examples=(400, 400),\n",
    "    intervention_fit_examples=160,\n",
    ")\n",
    "print(cfg)\n",
    "batch_size = cfg.batch_size\n",
    "\n",
    "model, tokenizer = load_model(\n",
    "    cfg.model, pad_token_id=cfg.pad_token_id, disable_exllama=False\n",
    ")\n",
    "print(model)\n",
    "\n",
    "N_train, N_test = cfg.max_examples\n",
    "N = sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
