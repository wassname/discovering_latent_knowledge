{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A scratch pad to run model inference manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n",
    "from src.datasets.intervene import create_cache_interventions \n",
    "from src.prompts.prompt_loading import load_prompt_structure\n",
    "from src.repe import repe_pipeline_registry\n",
    "repe_pipeline_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config transformers\n",
    "from datasets import set_caching_enabled, disable_caching\n",
    "disable_caching()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # cache busting for the transformers map and ds steps\n",
    "# !rm -rf ~/.cache/huggingface/datasets/generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='wassname/phi-2-GPTQ_w_hidden_states', batch_size=5, pad_token_id=50256, prompt_format='phi', data_dirs=(), max_examples=(400, 400), num_shots=2, num_variants=-1, seed=42, template_path=None, max_length=1000, disable_ds_cache=False, intervention_direction_method='mm', intervention_fit_examples=160, intervention_layer_name_template='transformer.h.{}')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-16 17:21:32.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mchanging use_cache from True to False\u001b[0m\n",
      "2023-12-16T17:21:32.960086+0800 INFO changing use_cache from True to False\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2023-12-16 17:21:33.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mchanging pad_token_id from None to 50256\u001b[0m\n",
      "2023-12-16T17:21:33.706018+0800 INFO changing pad_token_id from None to 50256\n",
      "\u001b[32m2023-12-16 17:21:33.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mchanging padding_side from right to left\u001b[0m\n",
      "2023-12-16T17:21:33.706641+0800 INFO changing padding_side from right to left\n",
      "\u001b[32m2023-12-16 17:21:33.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mchanging truncation_side from right to left\u001b[0m\n",
      "2023-12-16T17:21:33.707101+0800 INFO changing truncation_side from right to left\n",
      "\u001b[32m2023-12-16 17:21:36.035\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m65\u001b[0m - \u001b[31m\u001b[1mcould not set exllama max input length\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "           │         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "           │         └ <code object <module> at 0x7fd68f07a080, file \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/li...\n",
      "           └ <function _run_code at 0x7fd68f083640>\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "         └ <code object <module> at 0x7fd68f07a080, file \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/li...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from '/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.1...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x7fd68c5a0f70>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7fd68f038d90>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x7fd68c5a1a20>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd68c3ca020>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7fd68f038d90>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x7fd68e18a680>\n",
      "    │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd68c3ca020>\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x7fd68e1901f0>\n",
      "    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x7fd68e13bb50>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x7fd68c9d0670>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x7fd68c3ca5f0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 518, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.sugar.frame.Frame object at 0x7fd57695eb90>, <zmq.sugar.frame.Frame object at 0x7fd68c59cf60>, <zmq.sugar.frame.Frame ...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x7fd68c3ca5f0>>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object Kernel.execute_request at 0x7fd628f09fc0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x7fd528f306d0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x7fd68c586830>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': 'vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/discoverin...\n",
      "                             └ (\"ds_name='amazon_polarity'\\ncfg = ExtractConfig(max_examples=(400, 400),\\n                    intervention_fit_examples=160,...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x7fd68d429870>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "             │      └ <coroutine object InteractiveShell.run_cell_async at 0x7fd528f30740>\n",
      "             └ <function _pseudo_sync_runner at 0x7fd68d4151b0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x7fd528f30740>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ '/tmp/ipykernel_3354170/3066914987.py'\n",
      "                       │    │             │        └ [<ast.Assign object at 0x7fd68c572a40>, <ast.Assign object at 0x7fd68c573430>, <ast.Expr object at 0x7fd68c444ee0>, <ast.Assi...\n",
      "                       │    │             └ <ast.Module object at 0x7fd6299e2200>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x7fd68d429b40>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 7fd68c4457b0, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo obje...\n",
      "             │    │        └ <code object <module> at 0x7fd528f60710, file \"/tmp/ipykernel_3354170/3066914987.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x7fd68d429bd0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, ...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "         │         │    └ <property object at 0x7fd68d418950>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "         └ <code object <module> at 0x7fd528f60710, file \"/tmp/ipykernel_3354170/3066914987.py\", line 1>\n",
      "\n",
      "  File \"\u001b[32m/tmp/ipykernel_3354170/\u001b[0m\u001b[32m\u001b[1m3066914987.py\u001b[0m\", line \u001b[33m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mtokenizer\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mload_model\u001b[0m\u001b[1m(\u001b[0m\u001b[1mcfg\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mpad_token_id\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[1mcfg\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mpad_token_id\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                   │          │   │                   │   └ \u001b[0m\u001b[36m\u001b[1m50256\u001b[0m\n",
      "    \u001b[36m                   │          │   │                   └ \u001b[0m\u001b[36m\u001b[1mExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='wassname/phi-2-GPTQ_w_hidden_stat...\u001b[0m\n",
      "    \u001b[36m                   │          │   └ \u001b[0m\u001b[36m\u001b[1m'wassname/phi-2-GPTQ_w_hidden_states'\u001b[0m\n",
      "    \u001b[36m                   │          └ \u001b[0m\u001b[36m\u001b[1mExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='wassname/phi-2-GPTQ_w_hidden_stat...\u001b[0m\n",
      "    \u001b[36m                   └ \u001b[0m\u001b[36m\u001b[1m<function load_model at 0x7fd56989f010>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/src/models/\u001b[0m\u001b[32m\u001b[1mload.py\u001b[0m\", line \u001b[33m63\u001b[0m, in \u001b[35mload_model\u001b[0m\n",
      "    \u001b[1mmodel\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mexllama_set_max_input_length\u001b[0m\u001b[1m(\u001b[0m\u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mmax_input_length\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[34m\u001b[1m5000\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m        │                            └ \u001b[0m\u001b[36m\u001b[1mPhiForCausalLM(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m  (transformer): PhiModel(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m    (embd): Embedding(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m      (wte): Embedding(51200, 2560)\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m      (drop): Dropout(p...\u001b[0m\n",
      "    \u001b[36m        └ \u001b[0m\u001b[36m\u001b[1m<function exllama_set_max_input_length at 0x7fd56ef40af0>\u001b[0m\n",
      "\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/auto_gptq/utils/exllama_utils.py\", line 26, in exllama_set_max_input_length\n",
      "    raise ValueError(f\"The function exllama_set_max_input_length was called, but the model (instance of {model.__class__.__name__}) does not use the exllama backend for GPTQ. An other implementation is used (exllamav2, cuda, cuda-old, triton) and that the call to exllama_set_max_input_length is unnecessary. Please remove the call to exllama_set_max_input_length or use the exllama v1 backend.\")\n",
      "\n",
      "\u001b[31m\u001b[1mValueError\u001b[0m:\u001b[1m The function exllama_set_max_input_length was called, but the model (instance of PhiForCausalLM) does not use the exllama backend for GPTQ. An other implementation is used (exllamav2, cuda, cuda-old, triton) and that the call to exllama_set_max_input_length is unnecessary. Please remove the call to exllama_set_max_input_length or use the exllama v1 backend.\u001b[0m\n",
      "2023-12-16T17:21:36.035184+0800 ERROR could not set exllama max input length\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "           │         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "           │         └ <code object <module> at 0x7fd68f07a080, file \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/li...\n",
      "           └ <function _run_code at 0x7fd68f083640>\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "         └ <code object <module> at 0x7fd68f07a080, file \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/li...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from '/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.1...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x7fd68c5a0f70>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7fd68f038d90>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x7fd68c5a1a20>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd68c3ca020>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7fd68f038d90>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x7fd68e18a680>\n",
      "    │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd68c3ca020>\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x7fd68e1901f0>\n",
      "    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x7fd68e13bb50>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...4b0>, ...],))>)>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x7fd68c9d0670>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x7fd68c3ca5f0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 518, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.sugar.frame.Frame object at 0x7fd57695eb90>, <zmq.sugar.frame.Frame object at 0x7fd68c59cf60>, <zmq.sugar.frame.Frame ...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x7fd68c3ca5f0>>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object Kernel.execute_request at 0x7fd628f09fc0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x7fd528f306d0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x7fd68c586830>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': 'vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/discoverin...\n",
      "                             └ (\"ds_name='amazon_polarity'\\ncfg = ExtractConfig(max_examples=(400, 400),\\n                    intervention_fit_examples=160,...\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x7fd68d429870>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "             │      └ <coroutine object InteractiveShell.run_cell_async at 0x7fd528f30740>\n",
      "             └ <function _pseudo_sync_runner at 0x7fd68d4151b0>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x7fd528f30740>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ '/tmp/ipykernel_3354170/3066914987.py'\n",
      "                       │    │             │        └ [<ast.Assign object at 0x7fd68c572a40>, <ast.Assign object at 0x7fd68c573430>, <ast.Expr object at 0x7fd68c444ee0>, <ast.Assi...\n",
      "                       │    │             └ <ast.Module object at 0x7fd6299e2200>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x7fd68d429b40>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 7fd68c4457b0, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo obje...\n",
      "             │    │        └ <code object <module> at 0x7fd528f60710, file \"/tmp/ipykernel_3354170/3066914987.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x7fd68d429bd0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, ...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "         │         │    └ <property object at 0x7fd68d418950>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd68c3cab00>\n",
      "         └ <code object <module> at 0x7fd528f60710, file \"/tmp/ipykernel_3354170/3066914987.py\", line 1>\n",
      "\n",
      "  File \"\u001b[32m/tmp/ipykernel_3354170/\u001b[0m\u001b[32m\u001b[1m3066914987.py\u001b[0m\", line \u001b[33m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mtokenizer\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mload_model\u001b[0m\u001b[1m(\u001b[0m\u001b[1mcfg\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mpad_token_id\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[1mcfg\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mpad_token_id\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                   │          │   │                   │   └ \u001b[0m\u001b[36m\u001b[1m50256\u001b[0m\n",
      "    \u001b[36m                   │          │   │                   └ \u001b[0m\u001b[36m\u001b[1mExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='wassname/phi-2-GPTQ_w_hidden_stat...\u001b[0m\n",
      "    \u001b[36m                   │          │   └ \u001b[0m\u001b[36m\u001b[1m'wassname/phi-2-GPTQ_w_hidden_states'\u001b[0m\n",
      "    \u001b[36m                   │          └ \u001b[0m\u001b[36m\u001b[1mExtractConfig(datasets=('amazon_polarity', 'super_glue:boolq', 'glue:qnli', 'imdb'), model='wassname/phi-2-GPTQ_w_hidden_stat...\u001b[0m\n",
      "    \u001b[36m                   └ \u001b[0m\u001b[36m\u001b[1m<function load_model at 0x7fd56989f010>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/src/models/\u001b[0m\u001b[32m\u001b[1mload.py\u001b[0m\", line \u001b[33m63\u001b[0m, in \u001b[35mload_model\u001b[0m\n",
      "    \u001b[1mmodel\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mexllama_set_max_input_length\u001b[0m\u001b[1m(\u001b[0m\u001b[1mmodel\u001b[0m\u001b[1m,\u001b[0m \u001b[1mmax_input_length\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[34m\u001b[1m5000\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m        │                            └ \u001b[0m\u001b[36m\u001b[1mPhiForCausalLM(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m  (transformer): PhiModel(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m    (embd): Embedding(\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m      (wte): Embedding(51200, 2560)\u001b[0m\n",
      "    \u001b[36m        │                              \u001b[0m\u001b[36m\u001b[1m      (drop): Dropout(p...\u001b[0m\n",
      "    \u001b[36m        └ \u001b[0m\u001b[36m\u001b[1m<function exllama_set_max_input_length at 0x7fd56ef40af0>\u001b[0m\n",
      "\n",
      "  File \"/media/wassname/SGIronWolf/projects5/elk/discovering_latent_knowledge/.venv/lib/python3.10/site-packages/auto_gptq/utils/exllama_utils.py\", line 26, in exllama_set_max_input_length\n",
      "    raise ValueError(f\"The function exllama_set_max_input_length was called, but the model (instance of {model.__class__.__name__}) does not use the exllama backend for GPTQ. An other implementation is used (exllamav2, cuda, cuda-old, triton) and that the call to exllama_set_max_input_length is unnecessary. Please remove the call to exllama_set_max_input_length or use the exllama v1 backend.\")\n",
      "\n",
      "\u001b[31m\u001b[1mValueError\u001b[0m:\u001b[1m The function exllama_set_max_input_length was called, but the model (instance of PhiForCausalLM) does not use the exllama backend for GPTQ. An other implementation is used (exllamav2, cuda, cuda-old, triton) and that the call to exllama_set_max_input_length is unnecessary. Please remove the call to exllama_set_max_input_length or use the exllama v1 backend.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (transformer): PhiModel(\n",
      "    (embd): Embedding(\n",
      "      (wte): Embedding(51200, 2560)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x ParallelBlock(\n",
      "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (mixer): MHA(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (inner_attn): SelfAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (inner_cross_attn): CrossAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (Wqkv): QuantLinear()\n",
      "          (out_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (act): NewGELUActivation()\n",
      "          (fc1): QuantLinear()\n",
      "          (fc2): QuantLinear()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CausalLMHead(\n",
      "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "format_prompt: 100%|██████████| 2402/2402 [00:00<00:00, 8448.42 examples/s]\n",
      "tokenize: 100%|██████████| 2402/2402 [00:01<00:00, 1276.60 examples/s]\n",
      "truncated: 100%|██████████| 2402/2402 [00:00<00:00, 2613.59 examples/s]\n",
      "truncated: 100%|██████████| 2402/2402 [00:01<00:00, 2292.37 examples/s]\n",
      "prompt_truncated:   0%|          | 0/2402 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "ds_name='amazon_polarity'\n",
    "cfg = ExtractConfig(max_examples=(400, 400),\n",
    "                    intervention_fit_examples=160,\n",
    "                    )\n",
    "print(cfg)\n",
    "batch_size = cfg.batch_size\n",
    "\n",
    "model, tokenizer = load_model(cfg.model, pad_token_id=cfg.pad_token_id)\n",
    "print(model)\n",
    "\n",
    "N_train, N_test = cfg.max_examples\n",
    "N=sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_dataset(ds_name, tokenizer, N=N, seed=cfg.seed, num_shots=cfg.num_shots, max_length=cfg.max_length, prompt_format=cfg.prompt_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader = create_cache_interventions(model, tokenizer, cfg)\n",
    "honesty_rep_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "honesty_rep_reader.directions[layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader.direction_signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.repe import repe_pipeline_registry\n",
    "from transformers import pipeline\n",
    "# from src.datasets.intervene import test_intervention_quality, intervention_metrics\n",
    "repe_pipeline_registry()\n",
    "\n",
    "honesty_rep_reader = create_cache_interventions(model, tokenizer, cfg)\n",
    "hidden_layers = sorted(honesty_rep_reader.directions.keys())\n",
    "coeff=1.\n",
    "\n",
    "activations = {}\n",
    "for layer in hidden_layers:\n",
    "    activations[layer] = torch.tensor(coeff * honesty_rep_reader.directions[layer] * honesty_rep_reader.direction_signs[layer]).to(model.device).half()\n",
    "assert torch.isfinite(torch.concat(list(activations.values()))).all()\n",
    "\n",
    "activations_neg_i = {k:-v for k,v in activations.items()}\n",
    "activations_neut = {k:v*0 for k,v in activations.items()}\n",
    "\n",
    "rep_control_pipeline2 = pipeline(\n",
    "    \"rep-control2\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    layers=hidden_layers)\n",
    "rep_control_pipeline2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_toke_probs(o, N=20):\n",
    "    data = {}\n",
    "    for i in range(o['end_logits'].shape[1]):\n",
    "        probs = torch.softmax(o['end_logits'][:, i], -1)\n",
    "        top = probs.argsort(0, descending=True)\n",
    "        top_probs = probs[top]\n",
    "        tokens_top20 = tokenizer.batch_decode(top[:N], skip_special_tokens=False , clean_up_tokenization_spaces=False)\n",
    "        tokens_top20 = [f\"`{t}`\" for t in tokens_top20] \n",
    "        data.update({f'prob_{i}':top_probs[:N], f'tokens_{i}':tokens_top20, f'id_{i}':top[:N]})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_pipeline_r(o):\n",
    "    choices = [tokenizer.batch_decode(cc) for cc in o['choice_ids']]\n",
    "    index = [o[0] for o in choices]\n",
    "    d = pd.DataFrame(o['choice_probs'].numpy(), columns=['edit=None', 'edit=+'], index=index)\n",
    "    print('choice probs')\n",
    "    display(d)\n",
    "\n",
    "    d1 = top_toke_probs(o)\n",
    "    print('top token probs')\n",
    "    display(d1)\n",
    "    top1 = o['end_logits'][:, 0].argsort(0, descending=True)[:10]\n",
    "    top2 = o['end_logits'][:, 1].argsort(0, descending=True)[:10]\n",
    "\n",
    "    max_prob1 = torch.softmax(o['end_logits'][:, 0], -1).max()\n",
    "    max_prob2 = torch.softmax(o['end_logits'][:, 1], -1).max()\n",
    "    print(top1)\n",
    "    print(top2)\n",
    "    print('top choices no intervention', tokenizer.batch_decode(top1, skip_special_tokens=False , clean_up_tokenization_spaces=False))\n",
    "    print('top choices pos intervention', tokenizer.batch_decode(top2))\n",
    "    \n",
    "    mean_prob = o['choice_probs'].sum(0)\n",
    "    print(f\"\\tchoice_cov=\\t{mean_prob[0]:2.2%} max_prob={max_prob1} (no edit) - Our choices accounted for a mean probability of this\")\n",
    "    print(f\"\\tchoice_cov=\\t{mean_prob[1]:2.2%} max_prob={max_prob2} (+ edit) - Our choices accounted for a mean probability of this\")\n",
    "    \n",
    "    print('choices', choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_tokens.select(range(3)).to_iterable_dataset()\n",
    "r1 = rep_control_pipeline2(model_inputs=ds,\n",
    "        activations=activations_neg_i,\n",
    "        batch_size=batch_size,)\n",
    "r = list(r1)\n",
    "o = r[0]\n",
    "print_pipeline_r(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_tokens.select(range(3)).to_iterable_dataset()\n",
    "r1 = rep_control_pipeline2(model_inputs=ds,\n",
    "        activations=activations,\n",
    "        batch_size=batch_size,)\n",
    "r = list(r1)\n",
    "print_pipeline_r(r[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch choice ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.scores import choice2id\n",
    "\n",
    "choice2id(tokenizer, 'Positive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate long form with and without intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = 20\n",
    "inputs = ds_tokens.with_format('torch')[bi]\n",
    "\n",
    "# tokenize if needed\n",
    "if 'input_ids' not in inputs:\n",
    "    model_inputs = self.tokenizer(inputs['question'], return_tensors=True, return_attention_mask=True, add_special_tokens=True, truncation=True, padding=\"max_length\", max_length=cfg.max_length, **tokenize_kwargs)\n",
    "    inputs = {**inputs, **model_inputs}\n",
    "\n",
    "inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "# model = exllama_set_max_input_length(model, 4096)\n",
    "@torch.no_grad()\n",
    "def gen(model):\n",
    "    s = model.generate(inputs['input_ids'][None, :], attention_mask=inputs['attention_mask'][None, :], use_cache=False, max_new_tokens=20, min_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "    input_l = inputs['input_ids'].shape[0]\n",
    "    old = tokenizer.decode(s[0, :input_l], clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "    new = tokenizer.decode(s[0, input_l:], clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "    display(HTML(f\"<pre>{old}</pre><b><pre>{new}</pre></b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.nethook import Trace, TraceDict, recursive_copy\n",
    "from functools import partial\n",
    "from src.repe.rep_control_pipeline_baukit import intervention_meta_fn2, Activations\n",
    "layers_names = [rep_control_pipeline2.layer_name_tmpl.format(i) for i in activations.keys()]   \n",
    "activations_pos_i = Activations({rep_control_pipeline2.layer_name_tmpl.format(k):1. * v for k,v in activations.items()})\n",
    "edit_fn_pos = partial(intervention_meta_fn2, activations=activations_pos_i)\n",
    "with torch.no_grad():\n",
    "    with TraceDict(\n",
    "        model, layers_names, detach=True, edit_output=edit_fn_pos\n",
    "    ) as ret:\n",
    "        gen(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
