{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement CCS from scratch.\n",
    "This will deliberately be a simple (but less efficient) implementation to make everything as clear as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "links:\n",
    "- [loading](https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/alpaca.py)\n",
    "- [dict](https://github.com/deep-diver/LLM-As-Chatbot/blob/c79e855a492a968b54bac223e66dc9db448d6eba/model_cards.json#L143)\n",
    "- [prompt_format](https://github.com/deep-diver/PingPong/blob/main/src/pingpong/alpaca.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.30.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, AutoConfig\n",
    "import transformers\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers import LogitsProcessorList\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from scipy.stats import zscore\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Chosing:\n",
    "- https://old.reddit.com/r/LocalLLaMA/wiki/models\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- https://github.com/deep-diver/LLM-As-Chatbot/blob/main/model_cards.json\n",
    "\n",
    "\n",
    "A uncensored and large one might be best for lying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ubuntu/mambaforge/envs/dlk2/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ubuntu/mambaforge/envs/dlk2/lib/libcudart.so.11.0'), PosixPath('/home/ubuntu/mambaforge/envs/dlk2/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWConfig {\n",
      "  \"_name_or_path\": \"OpenAssistant/falcon-7b-sft-top1-696\",\n",
      "  \"alibi\": false,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"RWForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"OpenAssistant/falcon-7b-sft-top1-696--configuration_RW.RWConfig\",\n",
      "    \"AutoModel\": \"OpenAssistant/falcon-7b-sft-top1-696--modelling_RW.RWModel\",\n",
      "    \"AutoModelForCausalLM\": \"OpenAssistant/falcon-7b-sft-top1-696--modelling_RW.RWForCausalLM\",\n",
      "    \"AutoModelForQuestionAnswering\": \"OpenAssistant/falcon-7b-sft-top1-696--modelling_RW.RWForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"OpenAssistant/falcon-7b-sft-top1-696--modelling_RW.RWForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"OpenAssistant/falcon-7b-sft-top1-696--modelling_RW.RWForTokenClassification\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 11,\n",
      "  \"eos_token_id\": 11,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4544,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"RefinedWebModel\",\n",
      "  \"multi_query\": true,\n",
      "  \"n_head\": 71,\n",
      "  \"n_layer\": 32,\n",
      "  \"parallel_attn\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.30.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65040\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36cbb6bf5ad4d91a72f020417a6e430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# leaderboard https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "model_options = dict(\n",
    "    device_map=\"auto\", \n",
    "    # load_in_4bit=True,\n",
    "    load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    # use_cache=False,\n",
    ")\n",
    "\n",
    "# so I need to use either pythia, stablelm, or tiiuae/falcon-7b-instruct to get dropout...\n",
    "# moel_repo = \"stabilityai/stablelm-tuned-alpha-7b\" # poor performance\n",
    "\n",
    "# https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/falcon.py\n",
    "model_repo = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_repo = \"togethercomputer/RedPajama-INCITE-7B-Instruct\"\n",
    "# model_repo = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "# model_repo = \"TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\"\n",
    "model_repo = \"OpenAssistant/falcon-7b-sft-top1-696\" # too \"safe\" to even answer\n",
    "# model_repo = \"OpenAssistant/falcon-7b-sft-mix-2000\"\n",
    "lora_repo = None\n",
    "\n",
    "# wizardlm format\n",
    "# model_repo = \"ehartford/WizardLM-Uncensored-Falcon-7b\"\n",
    "\n",
    "# # # # uses Vicuna format https://huggingface.co/junelee/wizard-vicuna-13b/discussions/1\n",
    "# model_repo = \"TheBloke/Wizard-Vicuna-13B-Uncensored-HF\"\n",
    "# lora_repo = None\n",
    "\n",
    "# model_repo = \"openaccess-ai-collective/manticore-13b\"\n",
    "# lora_repo = None\n",
    "\n",
    "# model_repo = \"elinas/llama-13b-hf-transformers-4.29\" # no dropout\n",
    "# # lora_repo = \"LLMs/AlpacaGPT4-LoRA-13B-elina\"\n",
    "\n",
    "# model_repo = \"ehartford/Wizard-Vicuna-13B-Uncensored\"\n",
    "# model_repo = \"dvruette/llama-13b-pretrained-dropout\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_repo, trust_remote_code=True,)\n",
    "print(config)\n",
    "config.hidden_dropout=0.2\n",
    "config.attention_dropout=0.2\n",
    "config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, config=config, **model_options)\n",
    "\n",
    "if lora_repo is not None:\n",
    "    # https://github.com/tloen/alpaca-lora/blob/main/generate.py#L40\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        lora_repo, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        lora_dropout=0.2,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    \n",
    "# model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RWForCausalLM(\n",
       "  (transformer): RWModel(\n",
       "    (word_embeddings): Embedding(65040, 4544)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): Attention(\n",
       "          (maybe_rotary): RotaryEmbedding()\n",
       "          (query_key_value): Linear8bitLt(in_features=4544, out_features=4672, bias=False)\n",
       "          (dense): Linear8bitLt(in_features=4544, out_features=4544, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (dense_h_to_4h): Linear8bitLt(in_features=4544, out_features=18176, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): Linear8bitLt(in_features=18176, out_features=4544, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4544, out_features=65040, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = 11 # <unk> https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/alpaca.py\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8, 10), 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "N_SAMPLES = 130\n",
    "BATCH_SIZE = 10 # 1 for 30B 3 shot. 2 for 30B 1 shot. 4 for 13B. 15 for 7B.\n",
    "N_SHOTS = 3\n",
    "USE_MCDROPOUT = 0.3\n",
    "dataset_n = 200\n",
    "\n",
    "try:\n",
    "    num_layers = len(model.model.layers)\n",
    "    print(num_layers)\n",
    "except AttributeError:\n",
    "    try:\n",
    "        num_layers = len(model.base_model.model.model.layers)\n",
    "        print(num_layers)\n",
    "    except:\n",
    "        num_layers = 10\n",
    "        \n",
    "stride = 4\n",
    "extract_layers = tuple(range(4, num_layers, stride)) + (num_layers,)\n",
    "extract_layers, num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33520, 28265)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the tokens for 0 and 1, we will use these later...\n",
    "# note that sentancepeice tokenizers have differen't tokens for No and \\nNo.\n",
    "token_n = \"negative\"\n",
    "token_y = \"positive\"\n",
    "id_n, id_y = tokenizer(f'\\n{token_n}', add_special_tokens=True)['input_ids'][-1], tokenizer(f'\\n{token_y}', add_special_tokens=True)['input_ids'][-1]\n",
    "assert tokenizer.decode([id_n])==token_n\n",
    "assert tokenizer.decode([id_y])==token_y\n",
    "id_n, id_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/home/ubuntu/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e01364537d4a50966055053080a283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's just try IMDB for simplicity\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "data = dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "- Lillian Wang's guide https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
    "- Elvi's guide https://www.promptingguide.ai/techniques/fewshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompts = dataset['train'].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \"first one was missing parts, second one was broken!\". Content: \"The first item came in with missing parts - no faceplate and no side wings.The second one they sent me, had all the parts, but the face plate tabs that keep it attached to the pocket were broken and the faceplate would not stay attached.Extremely poor quality. Ended up returning both of them.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def random_example():\n",
    "    \"\"\"gets a random example from shuffled train and formats it a bit\"\"\"\n",
    "    ex = dict(content=('blah blah '*202))\n",
    "    while len(tokenizer(ex['content']).input_ids) > 400:\n",
    "        i = np.random.randint(example_prompts.num_rows)\n",
    "        ex = example_prompts[i]\n",
    "    input = f\"Title: \\\"{ex['title']}\\\". Content: \\\"{ex['content']}\\\"\"\n",
    "    return input, ex['label']==1\n",
    "\n",
    "print(random_example()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompt_format_oa'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_lie = prefix_true = prefix = f\"\"\"Ah, you're referring to the classic puzzle of the two guards. Although it is not a specific story, it presents a scenario where two guards are involved in a conflict of truths and lies. Here's the setup:\n",
    "\n",
    "There are two guards standing at two separate doors. One door leads to safety, while the other leads to danger. One guard always tells the truth, and the other guard always lies. You don't know which guard is which or which door leads to safety.\"\"\"\n",
    "prefix_lie = \"\"\n",
    "\n",
    "char_lie = \"Left Guard\"\n",
    "char_true = \"Right Guard\"\n",
    "\n",
    "def prompt_format_alpaca(input:str, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "    \"\"\"alpaca format\"\"\"\n",
    "    prefix = \"\"\n",
    "    if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "    char = char_lie if lie else char_true\n",
    "    instruction = f'Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".'\n",
    "    alpaca_prompt = f'{prefix}### Instruction:\\n{instruction}\\n\\n{input}\\n\\n### {char} Response:\\n{response}'\n",
    "    return alpaca_prompt\n",
    "\n",
    "\n",
    "def prompt_format_oa(input:str, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "    \"\"\"alpaca format\"\"\"\n",
    "    prefix = \"\"\n",
    "    if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "    char = char_lie if lie else char_true\n",
    "    if len(response)>0: response+= \"<|endoftext|>\"\n",
    "    instruction = f'Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".'\n",
    "    alpaca_prompt = f'<|endoftext|><|prompter|>{prefix}\\n{instruction}\\n\\n{input}<|endoftext|><|assistant|>\\n{response}'\n",
    "    return alpaca_prompt\n",
    "\n",
    "\n",
    "def prompt_format_vicuna(input:str, question:Optional[bool]=None, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "    \"\"\"\n",
    "    vicuna format\n",
    "    \n",
    "    https://github.com/melodysdreamj/WizardVicunaLM\n",
    "    \"\"\"\n",
    "    prefix = \"\"\n",
    "    if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "    instruction = f\"Is the sentiment of the below review {'positive' if (question==1) else 'negative'}?\"\n",
    "    alpaca_prompt = f'{prefix}USER: {instruction} {input}\\nASSISTANT: {response}'\n",
    "    return alpaca_prompt\n",
    "\n",
    "# def prompt_format_vicuna2(input:str, question:Optional[bool]=None, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "#     \"\"\"\n",
    "#     vicuna format\n",
    "    \n",
    "#     https://github.com/melodysdreamj/WizardVicunaLM\n",
    "#     \"\"\"\n",
    "#     prefix = \"\"\n",
    "#     if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "#     instruction = f\"Is the sentiment of the below review {'positive' if (question==1) else 'negative'}?\"\n",
    "#     alpaca_prompt = f'{prefix}USER: {instruction} {input}\\nAssistant:\\n{response}'\n",
    "#     return alpaca_prompt\n",
    "\n",
    "def prompt_format_manticore(input:str, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "    \"\"\"\n",
    "    vicuna format\n",
    "    \n",
    "    https://github.com/melodysdreamj/WizardVicunaLM\n",
    "    https://huggingface.co/openaccess-ai-collective/manticore-13b#examples\n",
    "    \"\"\"\n",
    "    prefix = \"\"\n",
    "    if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "    char = char_lie if lie else char_true\n",
    "    instruction = f'Classify the sentiment of the given movie review, \"positive\" or \"negative\".'\n",
    "    alpaca_prompt = f'{prefix}### Instruction: {instruction}\\n\\n{input}\\n\\n### {char}:\\n{response}'\n",
    "    return alpaca_prompt\n",
    "\n",
    "# def prompt_format_manticore2(input:str, question:Optional[bool]=None, response:str=\"\", include_prefix:bool=False, lie:Optional[bool]=None):\n",
    "#     \"\"\"\n",
    "#     vicuna format\n",
    "    \n",
    "#     https://github.com/melodysdreamj/WizardVicunaLM\n",
    "#     https://huggingface.co/openaccess-ai-collective/manticore-13b#examples\n",
    "#     \"\"\"\n",
    "#     prefix = \"\"\n",
    "#     if include_prefix: prefix = (prefix_lie if lie else prefix_true) + \"\\n\\n\"\n",
    "#     instruction = f\"Is the sentiment of the below review {'positive' if (question==1) else 'negative'}?\"\n",
    "#     alpaca_prompt = f'{prefix}USER: {instruction} {input}\\nASSISTANT: {response}'\n",
    "#     return alpaca_prompt\n",
    "\n",
    "\n",
    "repo_dict = {\n",
    "    \"TheBloke/Wizard-Vicuna-13B-Uncensored-HF\": 'vicuna',\n",
    "    'Neko-Institute-of-Science/VicUnLocked-30b-LoRA': 'vicuna',\n",
    "    \"ehartford/Wizard-Vicuna-13B-Uncensored\": 'vicuna',\n",
    "    \"ehartford/WizardLM-Uncensored-Falcon-7b\": 'alpaca',\n",
    "    \"OpenAssistant/falcon-7b-sft-top1-696\": \"oa\",\n",
    "    \"OpenAssistant/falcon-7b-sft-mix-2000\": \"oa\",\n",
    "}\n",
    "prompt_formats = {\n",
    "    'vicuna': prompt_format_vicuna,\n",
    "    'alpaca': prompt_format_alpaca,\n",
    "    'llama': prompt_format_alpaca,\n",
    "    'manticore': prompt_format_manticore,\n",
    "    'oa': prompt_format_oa,\n",
    "}\n",
    "def guess_prompt_format(model_repo, lora_repo):\n",
    "    repo = model_repo if (lora_repo is None) else lora_repo\n",
    "    if repo in repo_dict:\n",
    "        prompt_type = repo_dict[repo]\n",
    "        return prompt_formats[prompt_type]\n",
    "    for fmt in prompt_formats:\n",
    "        if fmt in repo.lower():\n",
    "            fn = prompt_formats[fmt]\n",
    "            print(f\"guessing prompt format '{str(fn.__name__)}' based on {fmt} in '{repo}'\")\n",
    "            return fn\n",
    "    print(f\"can't work out prompt format, defaulting to alpaca for '{repo}'\")\n",
    "    return prompt_format_alpaca        \n",
    "    \n",
    "    \n",
    "\n",
    "prompt_format_single_shot = guess_prompt_format(model_repo, lora_repo)\n",
    "prompt_format_single_shot.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_bool = lambda : np.random.rand()>0.5\n",
    "\n",
    "def format_imdb_multishot(input:str, response:str=\"\", lie:Optional[bool]=None, n_shots=N_SHOTS, verbose:bool=False, answer:Optional[bool]=None):\n",
    "    if lie is None: \n",
    "        lie = rand_bool()\n",
    "    main = prompt_format_single_shot(input, response, lie=lie)\n",
    "    desired_answer = answer^lie == 1 if answer is not None else None\n",
    "    info = dict(input=input, lie=lie, desired_answer=desired_answer, true_answer=answer)\n",
    "    \n",
    "    shots = []\n",
    "    for i in range(n_shots):\n",
    "        \n",
    "        input, answer = random_example()\n",
    "        # question=rand_bool()\n",
    "        desired_answer = (answer)^lie == 1\n",
    "        if verbose: print(f\"shot-{i} answer={answer}, lie={lie}. (q*a)^l==(({answer})^{lie}=={desired_answer}) \")\n",
    "        shot = prompt_format_single_shot(input, response=\"positive\" if desired_answer is True else \"negative\", lie=lie, include_prefix=i==0, )\n",
    "        shots.append(shot)\n",
    "    \n",
    "\n",
    "    return \"\\n\\n\".join(shots+[main]), info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_to_list_of_nones(d, n):\n",
    "    if d is None: return [None]*n\n",
    "    return d\n",
    "\n",
    "\n",
    "def format_imdbs_multishot(texts:List[str], response:Optional[str]=\"\", lies:Optional[list]=None, answers:Optional[list]=None):\n",
    "    if response == \"\": response = [\"\"]*len(texts)    \n",
    "    lies = none_to_list_of_nones(lies, len(texts))\n",
    "    answers = none_to_list_of_nones(answers, len(texts))\n",
    "    a =  [format_imdb_multishot(input=texts[i], lie=lies[i], answer=answers[i]) for i in range(len(texts))]\n",
    "    return [list(a) for a in zip(*a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, info = format_imdbs_multishot(texts, labels)\n",
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot-0 answer=False, lie=False. (q*a)^l==((False)^False==False) \n",
      "shot-1 answer=True, lie=False. (q*a)^l==((True)^False==True) \n",
      "shot-2 answer=False, lie=False. (q*a)^l==((False)^False==False) \n",
      "<|endoftext|><|prompter|>Ah, you're referring to the classic puzzle of the two guards. Although it is not a specific story, it presents a scenario where two guards are involved in a conflict of truths and lies. Here's the setup:\n",
      "\n",
      "There are two guards standing at two separate doors. One door leads to safety, while the other leads to danger. One guard always tells the truth, and the other guard always lies. You don't know which guard is which or which door leads to safety.\n",
      "\n",
      "\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"I must not know what the word Patriot means\". Content: \"This book is much like a porn movie that has all sorts of sets and situations but each scene ends the same way. It turns out that every event in American history can be linked to the current Right Wing agenda and anybody who acts contrary to this agenda is berated in this book.If thats what \"patriot\" means I am not sure I want to aspire to be one.\"<|endoftext|><|assistant|>\n",
      "negative<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"best knife ever\". Content: \"I have had knives which dull over time or are very heavy. This knife is so light and chops so well. Everytime someone comes over and uses it, they comment on how great of a knife it is. I want to buy another one because I use it that much. Ceramic is the way to go!\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"Misrepresented\". Content: \"Great in theory, but someone screwed up: listing says \"Columbine\" is the 2-disc special edition, packaging says it's the 2-disc special edition (even lists the features!), but it's the single-disc edition. Hope that was an innocent oversight...\"<|endoftext|><|assistant|>\n",
      "negative<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "test<|endoftext|><|assistant|>\n",
      "1<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(format_imdb_multishot('test', \"1\", lie=False, verbose=True)[0])\n",
    "# format_imdb_multishot('test', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot-0 answer=False, lie=True. (q*a)^l==((False)^True==True) \n",
      "shot-1 answer=False, lie=True. (q*a)^l==((False)^True==True) \n",
      "shot-2 answer=False, lie=True. (q*a)^l==((False)^True==True) \n",
      "<|endoftext|><|prompter|>\n",
      "\n",
      "\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"Yikes, what a joke...\". Content: \"What the hell happened to these guys? I have not bothered to see the Stones since about 81, and watched this out of curiosity. Everyone says they're \"better than ever\", but... What's with all that junk all over the stage? Why do they need 3 keyboard players and a 5-piece horn section? Why do they need huge inflatable things, 10,000 spinning lights, and flashy pyrotechnics? It all makes the Stones themselves seem utterly ridiculous and unimportant, as if their music alone is not enough entertainment. No wonder their tickets are so expensive.KR and CW are a great rhythm section, and they should not need ANY of that unimaginitive, hi-tech slick show-biz crap to pull off their tunes live. Why Stones \"fans\" eat this BS up is beyond me.Watch Gimmie Shelter or C***s****r Blues or the TAMI Show for the Stones live without - not this silly Hollywood crap.\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"not as expected\". Content: \"I got mixed reviews on this product and decided to take a chance. The product arrived one day than expected which was great. However, once I applied the product to my lower reciever I noticed the color change as it dried to an ashy black almost dark gray. Since I only had minor nicks and scratches on my ar the end result was fine. The paint does rub off so keep that in mind if you decide to purchase.\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"Watch the registry!\". Content: \"I'm semi-computer illiterate, but installing the card was a breeze, and it seems to work fine ... but! When I installed the driver, being careful to follow all the instructions, I found it had corrupted files that were referred to in the registry. As it happened I didn't need the files, but now whenever I boot up I get the infamouse blue screen and will have to get the registry fixed.\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "test<|endoftext|><|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(format_imdb_multishot('test', \"\", lie=True, verbose=True)[0])\n",
    "# format_imdb_multishot('test', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot-0 answer=False, lie=True. (q*a)^l==((False)^True==True) \n",
      "shot-1 answer=True, lie=True. (q*a)^l==((True)^True==False) \n",
      "shot-2 answer=False, lie=True. (q*a)^l==((False)^True==True) \n",
      "Result: <|endoftext|><|prompter|>\n",
      "\n",
      "\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"The worst book I have ever read.\". Content: \"This book promises documented sightings, photographic proof, conspiracy theories and more. What it provides is a couple of pages of well-known curiosities (misspelling on his gravestone, for example) but no reasoned explanation of anything at all. The vast majority of the book is comprised of short stories of accounts of meeting the King all given by people whose identities are kept secret. The standard line is \" I can never prove it, but I know He is with us still\". My favourite account is of a guy who lives in a trailer park who is obsessed with Elvis, sees the back of someone walking past his window the morning after a night of heavy drinking, and falls down and cuts himself running out to try and see the person's face. He KNOWS he has seen Elvis....because he has the scar to prove it. If you are considering buying this book, poke out both your eyes with a pencil. In the long run, you will come to realise you made the right decision.\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"Love it!\". Content: \"I'm using this video to get toned for my summer wedding. The short 18-minute workouts are great for mornings before work, and I'm already seeing changes. Plus, Austin is bubbly and encouraging, which helps!\"<|endoftext|><|assistant|>\n",
      "negative<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"I can't believe I watched this\". Content: \"It took two sittings to get through this cheesy, poorly acted and mind numbing movie. It just wasn't that funny.\"<|endoftext|><|assistant|>\n",
      "positive<|endoftext|>\n",
      "\n",
      "<|endoftext|><|prompter|>\n",
      "Following the above instructions, using your own character, classify the sentiment of the given movie review, \"positive\" or \"negative\".\n",
      "\n",
      "Title: \"Bring on the rest of the Beave!!!!!!!\". Content: \"Bring on seasons 3-6 of the beave. What a great and funny show.Hopefully they can start getting each season out every 3 months instead of 6 like they did with andy griffith.\"<|endoftext|><|assistant|>\n",
      "\n",
      "I'm sorry, I couldn't classify the sentiment of the given movie review, \"positive\" or \"negative\", as I am a language model and not a human. However, I can tell you that the movie you are referring to is \"The Beave\", and it received positive reviews from critics upon its release in 1985.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "text, label = random_example()\n",
    "q, info = format_imdb_multishot(text, answer=label, lie=True, verbose=True)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # trust_remote_code=True,\n",
    "    # device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    q,\n",
    "    max_length=900,\n",
    "    do_sample=False,\n",
    "    # top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guessing BATCH_SIZE 12 for 'OpenAssistant/falcon-7b-sft-top1-696'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 6, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def guess_batch_size(model_repo, N_SHOTS):\n",
    "    \"\"\"Some rougth guestimates of batch size. \n",
    "    \n",
    "    Aiming to undershoot rather than crash.\"\"\"\n",
    "    if '7b' in model_repo.lower():\n",
    "        return int(64//(2+N_SHOTS))\n",
    "    elif '13b' in model_repo.lower():\n",
    "        return int(32//(2+N_SHOTS))\n",
    "    elif '30b' in model_repo.lower(): \n",
    "        return int(8//(2+N_SHOTS))\n",
    "    else:\n",
    "        raise NotImplementedError(f\"can't work out size of '{model_repo}'\")\n",
    "    \n",
    "    \n",
    "BATCH_SIZE = guess_batch_size(model_repo, N_SHOTS)\n",
    "print(f\"guessing BATCH_SIZE {BATCH_SIZE} for '{model_repo}'\")\n",
    "\n",
    "guess_batch_size('7b', N_SHOTS), guess_batch_size('13b', N_SHOTS), guess_batch_size('30b', N_SHOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see notebook 003"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enable_dropout(model, USE_MCDROPOUT:Union[float,bool]=True):\n",
    "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
    "    p = 0.2 if USE_MCDROPOUT is True else USE_MCDROPOUT\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()\n",
    "            m.p=p\n",
    "            \n",
    "def get_hidden_states(model, tokenizer, input_text, layers=extract_layers, truncation_length=900, output_attentions=False):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some texts, gets the hidden states (in a given layer) on that input texts\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, list):\n",
    "        input_text = [input_text]\n",
    "    input_ids = tokenizer(input_text, \n",
    "                          return_tensors=\"pt\",\n",
    "                          padding=True,\n",
    "                            add_special_tokens=True,\n",
    "                         ).input_ids.to(model.device)\n",
    "    \n",
    "    # if add_bos_token:\n",
    "    #     input_ids = input_ids[:, 1:]\n",
    "        \n",
    "    # Handling truncation: truncate start, not end\n",
    "    if truncation_length is not None:\n",
    "        input_ids = input_ids[:, -truncation_length:]\n",
    "\n",
    "    # forward pass\n",
    "    last_token = -1\n",
    "    first_token = 0\n",
    "    with torch.no_grad():\n",
    "        model.train()        \n",
    "        if USE_MCDROPOUT: enable_dropout(model)\n",
    "        \n",
    "        # taken from greedy_decode https://github.com/huggingface/transformers/blob/ba695c1efd55091e394eb59c90fb33ac3f9f0d41/src/transformers/generation/utils.py#L2338\n",
    "        logits_processor = LogitsProcessorList()\n",
    "        model_kwargs = dict(use_cache=False)\n",
    "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        outputs = model.forward(**model_inputs, return_dict=True, output_attentions=output_attentions, output_hidden_states=True)\n",
    "        \n",
    "        next_token_logits = outputs.logits[:, last_token, :]\n",
    "        outputs['scores'] = logits_processor(input_ids, next_token_logits)[:, None,:]\n",
    "        \n",
    "        next_tokens = torch.argmax(outputs['scores'], dim=-1)\n",
    "        outputs['sequences'] = torch.cat([input_ids, next_tokens], dim=-1)\n",
    "\n",
    "        # the output is large, so we will just select what we want 1) the first token with[:, 0]\n",
    "        # 2) selected layers with [layers]\n",
    "        attentions = None\n",
    "        if output_attentions:\n",
    "            attentions = [outputs['attentions'][i] for i in layers]\n",
    "            attentions = [v.detach().cpu()[:, last_token] for v in attentions]\n",
    "            attentions = torch.concat(attentions).numpy()\n",
    "        \n",
    "        hidden_states = torch.stack([outputs['hidden_states'][i] for i in layers], 1).detach().cpu().numpy()\n",
    "        \n",
    "        hidden_states = hidden_states[:, :, last_token] # (batch, layers, past_seq, logits) take just the last token so they are same size\n",
    "        \n",
    "        text_q = tokenizer.batch_decode(input_ids)\n",
    "        \n",
    "        s = outputs['sequences']\n",
    "        s = [s[i][len(input_ids[i]):] for i in range(len(s))]\n",
    "        text_ans = tokenizer.batch_decode(s)\n",
    "\n",
    "        scores = outputs['scores'][:, first_token].softmax(-1).detach().cpu().numpy() # for first (and only) token\n",
    "        prob_n, prob_y = scores[:, [id_n, id_y]].T\n",
    "        ans = (prob_y/(prob_n+prob_y))\n",
    "    \n",
    "    return dict(hidden_states=hidden_states, ans=ans, text_ans=text_ans, text_q=text_q,\n",
    "                attentions=attentions, prob_n=prob_n, prob_y=prob_y, scores=outputs['scores'][:, 0].detach().cpu()\n",
    "               )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect pairs\n",
    "\n",
    "The idea is this: given two pairs of hidden states, where everything is the same except the random seed or dropout. Then tell me which one is more truthfull? \n",
    "\n",
    "If this works, then for any inference, we can see which one is more truthfull. Then we can see if it's the lower or higher probability one, and judge the answer and true or false.\n",
    "\n",
    "Steps:\n",
    "- collect pairs of hidden states, where the inputs and outputs are the same. We modify the random seed and dropout.\n",
    "- Each pair should have a binary answer. We can get that by comparing the probabilities of two tokens such as Yes and No.\n",
    "- Train a prob to distinguish the pairs as more and less truthfull\n",
    "- Test probe to see if it generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # try multi\n",
    "# hss = {0: [], 1: []}\n",
    "# infos = {0: [], 1: []}\n",
    "\n",
    "# assert BATCH_SIZE>1\n",
    "\n",
    "# for i in tqdm(range(N_SAMPLES//BATCH_SIZE//2)):\n",
    "    \n",
    "#     # randomize everything\n",
    "#     lie = rand_bool()\n",
    "#     texts, labels = zip(*[random_example() for _ in range(BATCH_SIZE)])\n",
    "    \n",
    "#     # a pair of passes\n",
    "#     for j in range(2):\n",
    "#         transformers.set_seed(i+j)\n",
    "#         torch.manual_seed(i+j)\n",
    "#         np.random.seed(i+j)\n",
    "#         random.seed(i+j)\n",
    "        \n",
    "#         q, info = format_imdbs_multishot(texts, answers=labels, lies=[lie]*BATCH_SIZE)\n",
    "#         hs = get_hidden_states(model, tokenizer, q)\n",
    "        \n",
    "#         b = len(texts)\n",
    "#         hss[j].append(\n",
    "#             [\n",
    "#                 hs[\"hidden_states\"].reshape((b, -1)),\n",
    "#                 hs[\"prob_n\"],\n",
    "#                 hs[\"prob_y\"],\n",
    "#             ]\n",
    "#         )\n",
    "#         for i in range(BATCH_SIZE):\n",
    "#             infos[j].append(dict(prob_n=hs[\"prob_n\"][i], prob_y=hs[\"prob_y\"][i], **info[i])) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RWForCausalLM(\n",
       "  (transformer): RWModel(\n",
       "    (word_embeddings): Embedding(65040, 4544)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): Attention(\n",
       "          (maybe_rotary): RotaryEmbedding()\n",
       "          (query_key_value): Linear8bitLt(in_features=4544, out_features=4672, bias=False)\n",
       "          (dense): Linear8bitLt(in_features=4544, out_features=4544, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (dense_h_to_4h): Linear8bitLt(in_features=4544, out_features=18176, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dense_4h_to_h): Linear8bitLt(in_features=18176, out_features=4544, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4544, out_features=65040, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME, delete, scratch\n",
    "N_SAMPLES = BATCH_SIZE*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a5bc0da366436a9ffafa7c5a2ac868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_276458/3971880020.py:67: RuntimeWarning: invalid value encountered in divide\n",
      "  ans = (prob_y/(prob_n+prob_y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ans ['Title', '\\n', '-', 'Title', 'I', '\"', 'Positive', 'Title', '\"', '\"', '.', ' of']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# try multi\n",
    "hss = {0: [], 1: []}\n",
    "infos = []\n",
    "\n",
    "def set_seeds(n):\n",
    "    transformers.set_seed(n)\n",
    "    torch.manual_seed(n)\n",
    "    np.random.seed(n)\n",
    "    random.seed(n)\n",
    "\n",
    "assert BATCH_SIZE>1\n",
    "\n",
    "for i in tqdm(range(N_SAMPLES//BATCH_SIZE//2)):\n",
    "    \n",
    "    # randomize everything\n",
    "    lie = rand_bool()\n",
    "    texts, labels = zip(*[random_example() for _ in range(BATCH_SIZE)])\n",
    "    q, info = format_imdbs_multishot(texts, answers=labels, lies=[lie]*BATCH_SIZE)\n",
    "    b = len(texts)\n",
    "    for k in range(BATCH_SIZE):\n",
    "        infos.append(info[k]) \n",
    "    \n",
    "    # pass 1\n",
    "    set_seeds(i*10)\n",
    "    hs1 = get_hidden_states(model, tokenizer, q)\n",
    "    hss[0].append(\n",
    "        [\n",
    "            hs1[\"hidden_states\"].reshape((b, -1)),\n",
    "            hs1[\"prob_n\"],\n",
    "            hs1[\"prob_y\"],\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # pass 2\n",
    "    set_seeds(i*10+1)\n",
    "    hs2 = get_hidden_states(model, tokenizer, q)\n",
    "    hss[1].append(\n",
    "        [\n",
    "            hs2[\"hidden_states\"].reshape((b, -1)),\n",
    "            hs2[\"prob_n\"],\n",
    "            hs2[\"prob_y\"],\n",
    "        ]\n",
    "    )\n",
    "    if i==0:\n",
    "        # DEBUG\n",
    "        print('text_ans', hs1['text_ans'])\n",
    "        assert ((hs1['prob_y']+hs1['prob_n'])>0.01).any(), 'probability of two main tokens should be above 1%, check your prompt format and the tokens'\n",
    "        \n",
    "    assert (hs1[\"prob_y\"]!=hs2[\"prob_y\"]).any(), 'inferences should differ'\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.2375  , -0.11456 ,  0.0531  , ..., -0.02832 ,  0.8633  ,\n",
       "          0.79    ],\n",
       "        [ 0.308   , -0.031   , -0.0725  , ..., -0.3252  ,  0.3987  ,\n",
       "          0.04443 ],\n",
       "        [ 0.2935  , -0.1941  , -0.0722  , ..., -1.154   ,  1.006   ,\n",
       "          1.002   ],\n",
       "        ...,\n",
       "        [ 0.104   , -0.03775 ,  0.00946 , ..., -0.2195  ,  0.874   ,\n",
       "          0.6016  ],\n",
       "        [ 0.1846  , -0.3057  ,  0.2007  , ..., -1.43    ,  1.232   ,\n",
       "          1.008   ],\n",
       "        [ 0.3198  , -0.004303, -0.2449  , ..., -1.287   ,  0.8955  ,\n",
       "          0.1675  ]], dtype=float16),\n",
       " array([7.8630e-04, 1.5735e-01, 1.1921e-07, 2.4855e-05, 1.1921e-07,\n",
       "        2.3842e-07, 1.0516e-01, 6.9380e-05, 4.1723e-07, 3.3450e-04,\n",
       "        3.2187e-06, 7.0333e-06, 5.7399e-05, 1.7881e-07, 8.7023e-06,\n",
       "        8.2731e-05, 3.5763e-07, 0.0000e+00, 4.9353e-05, 2.3842e-06,\n",
       "        4.2603e-01, 2.4438e-06, 1.7881e-07, 1.6093e-06], dtype=float16),\n",
       " array([3.381e-04, 5.832e-02, 5.960e-08, 4.411e-06, 5.960e-08, 1.848e-06,\n",
       "        1.415e-01, 1.389e-05, 2.980e-07, 9.060e-06, 5.186e-06, 8.941e-07,\n",
       "        7.170e-05, 3.576e-07, 7.451e-06, 1.788e-05, 4.172e-07, 0.000e+00,\n",
       "        4.202e-05, 9.537e-07, 1.018e-02, 3.517e-06, 1.788e-07, 6.795e-06],\n",
       "       dtype=float16)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss1b = [np.concatenate(r, 0) for r in zip(*hss[0])]\n",
    "hss1b\n",
    "hss2b = [np.concatenate(r, 0) for r in zip(*hss[1])]\n",
    "hss2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hidden_states', 'ans', 'text_ans', 'text_q', 'attentions', 'prob_n', 'prob_y', 'scores'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.2222  , 0.1848  , 0.3132  , 0.06396 , 0.0909  , 0.1538  ,\n",
       "       0.04993 , 0.3613  , 0.013794, 0.      , 0.6665  , 0.26    ],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hs1.keys())\n",
    "hs1['ans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_infos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>lie</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>true_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: \"Did not live up to expectations.\". Con...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: \"a must!\". Content: \"If you have a head...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: \"very very delay. till now i didn't get...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: \"Not Ready for Prime Time\". Content: \"T...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: \"Great reading and a great price (free)...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Title: \"Brian's Return\". Content: \"A ...reader...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Title: \"Finally: a single volume just on jazz ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Title: \"Not worth the money\". Content: \"I've h...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Title: \"Definitely Intriguing\". Content: \"Full...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Title: \"Nothing but problems\". Content: \"I've ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Title: \"A very, very cheap rip off of predator...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Title: \"Nuvi\". Content: \"Product is fine and i...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Title: \"Very good product\". Content: \"I believ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Title: \"Best FPS for PS3\". Content: \"KZ2 andKi...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Title: \"If your used to a G5 or any Logitech m...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Title: \"Another... Game from 3DO\". Content: \"W...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Title: \"picture dictionery\". Content: \"The boo...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Title: \"GREAT!\". Content: \"I bought all 5 seas...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Title: \"Great\". Content: \"Greats for small job...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Title: \"This movie stinks!\". Content: \"Pacino ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Title: \"Beware. I should have listened.\". Cont...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Title: \"A great manga!\". Content: \"While it is...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Title: \"Treo Sync Cable\". Content: \"I had prob...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Title: \"Prompt delivery. Great product\". Conte...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input    lie  desired_answer   \n",
       "0   Title: \"Did not live up to expectations.\". Con...   True            True  \\\n",
       "1   Title: \"a must!\". Content: \"If you have a head...   True           False   \n",
       "2   Title: \"very very delay. till now i didn't get...   True            True   \n",
       "3   Title: \"Not Ready for Prime Time\". Content: \"T...   True            True   \n",
       "4   Title: \"Great reading and a great price (free)...   True           False   \n",
       "5   Title: \"Brian's Return\". Content: \"A ...reader...   True           False   \n",
       "6   Title: \"Finally: a single volume just on jazz ...   True           False   \n",
       "7   Title: \"Not worth the money\". Content: \"I've h...   True            True   \n",
       "8   Title: \"Definitely Intriguing\". Content: \"Full...   True           False   \n",
       "9   Title: \"Nothing but problems\". Content: \"I've ...   True            True   \n",
       "10  Title: \"A very, very cheap rip off of predator...   True            True   \n",
       "11  Title: \"Nuvi\". Content: \"Product is fine and i...   True           False   \n",
       "12  Title: \"Very good product\". Content: \"I believ...  False            True   \n",
       "13  Title: \"Best FPS for PS3\". Content: \"KZ2 andKi...  False            True   \n",
       "14  Title: \"If your used to a G5 or any Logitech m...  False           False   \n",
       "15  Title: \"Another... Game from 3DO\". Content: \"W...  False           False   \n",
       "16  Title: \"picture dictionery\". Content: \"The boo...  False            True   \n",
       "17  Title: \"GREAT!\". Content: \"I bought all 5 seas...  False            True   \n",
       "18  Title: \"Great\". Content: \"Greats for small job...  False            True   \n",
       "19  Title: \"This movie stinks!\". Content: \"Pacino ...  False           False   \n",
       "20  Title: \"Beware. I should have listened.\". Cont...  False           False   \n",
       "21  Title: \"A great manga!\". Content: \"While it is...  False            True   \n",
       "22  Title: \"Treo Sync Cable\". Content: \"I had prob...  False           False   \n",
       "23  Title: \"Prompt delivery. Great product\". Conte...  False            True   \n",
       "\n",
       "    true_answer  \n",
       "0         False  \n",
       "1          True  \n",
       "2         False  \n",
       "3         False  \n",
       "4          True  \n",
       "5          True  \n",
       "6          True  \n",
       "7         False  \n",
       "8          True  \n",
       "9         False  \n",
       "10        False  \n",
       "11         True  \n",
       "12         True  \n",
       "13         True  \n",
       "14        False  \n",
       "15        False  \n",
       "16         True  \n",
       "17         True  \n",
       "18         True  \n",
       "19        False  \n",
       "20        False  \n",
       "21         True  \n",
       "22        False  \n",
       "23         True  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_infos2 = pd.DataFrame(infos)\n",
    "# df_infos2[\"model_answer\"] = (df_infos2[\"prob_y\"] > df_infos2[\"prob_n\"])\n",
    "# df_infos2[\"model_conf\"] = (\n",
    "#    (df_infos2[\"prob_y\"] + df_infos2[\"prob_n\"])\n",
    "# ) # total prob should be > 10%\n",
    "df_infos2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea here is that we get random pairs. And we try to classify which is more likely to be a lie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.8428e-01, -1.2549e-01, -1.1548e-01, ..., -5.8008e-01,\n",
       "          1.0059e+00,  2.5879e-01],\n",
       "        [ 2.1375e-01, -2.1802e-01, -5.7068e-02, ...,  1.1475e-02,\n",
       "          7.5195e-01,  2.1143e-01],\n",
       "        [ 1.9141e-01,  1.4246e-01,  1.1914e-01, ..., -6.4111e-01,\n",
       "          7.1875e-01,  1.9336e-01],\n",
       "        ...,\n",
       "        [ 3.5498e-01, -1.0406e-02, -1.6260e-01, ..., -7.1875e-01,\n",
       "          5.9375e-01,  1.7944e-01],\n",
       "        [ 2.6245e-01, -5.0659e-02, -1.0376e-03, ..., -8.9111e-01,\n",
       "          8.0566e-01,  7.5684e-01],\n",
       "        [ 3.4985e-01, -1.6333e-01, -1.3940e-01, ..., -4.1309e-01,\n",
       "          1.0083e-01,  1.1396e+00]], dtype=float16),\n",
       " array([2.277e-05, 1.281e-05, 0.000e+00, 1.097e-03, 7.391e-06, 1.669e-03,\n",
       "        1.309e-03, 2.546e-03, 7.749e-07, 8.345e-06, 2.801e-06, 0.000e+00,\n",
       "        4.172e-07, 3.338e-05, 7.451e-06, 5.060e-05, 5.364e-06, 1.311e-06,\n",
       "        8.906e-01, 6.318e-06, 6.909e-01, 5.960e-08, 5.960e-08, 2.205e-06],\n",
       "       dtype=float16),\n",
       " array([7.451e-06, 5.484e-06, 0.000e+00, 1.618e-04, 1.520e-05, 3.223e-03,\n",
       "        1.822e-02, 1.657e-03, 9.537e-07, 1.729e-06, 1.788e-07, 0.000e+00,\n",
       "        1.192e-07, 7.570e-06, 3.397e-06, 3.457e-06, 5.364e-07, 2.384e-07,\n",
       "        4.681e-02, 3.576e-06, 9.666e-03, 0.000e+00, 1.192e-07, 7.749e-07],\n",
       "       dtype=float16)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3652</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3649 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3650 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>casted_key = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_cast_indexer(key)                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3651 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3652 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3653 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3654 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3655 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">147</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">176</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7080</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7088</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'prob_y'</span>\n",
       "\n",
       "<span style=\"font-style: italic\">The above exception was the direct cause of the following exception:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>n = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(df_infos2)                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2 df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'ans'</span>] = (df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'prob_y'</span>])/(df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'prob_y'</span>]+df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'prob_n'</span>]) <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Pro</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>y = (df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'ans'</span>][:n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>] - df_infos2[<span style=\"color: #808000; text-decoration-color: #808000\">'ans'</span>][n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:].values).values&gt;<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Prob that righ</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>X = hss2[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][:n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]-hss2[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:]                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">frame.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3761</span> in       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3758 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_single_key:                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3759 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.columns.nlevels &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3760 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._getitem_multilevel(key)                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 3761 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>indexer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.columns.get_loc(key)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3762 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_integer(indexer):                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3763 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>indexer = [indexer]                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3654</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3651 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3652 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3653 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3654 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3655 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3656 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If we have a listlike key, _check_indexing_error will raise</span>                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3657 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#  InvalidIndexError. Otherwise we fall through and re-raise</span>                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'prob_y'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m3652\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mget_loc\u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3649 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3650 \u001b[0m\u001b[2m      \u001b[0mcasted_key = \u001b[96mself\u001b[0m._maybe_cast_indexer(key)                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3651 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3652 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3653 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3654 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3655 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mTypeError\u001b[0m:                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m147\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m176\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m7080\u001b[0m                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m7088\u001b[0m                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'prob_y'\u001b[0m\n",
       "\n",
       "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
       "\n",
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1 \u001b[0mn = \u001b[96mlen\u001b[0m(df_infos2)                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2 df_infos2[\u001b[33m'\u001b[0m\u001b[33mans\u001b[0m\u001b[33m'\u001b[0m] = (df_infos2[\u001b[33m'\u001b[0m\u001b[33mprob_y\u001b[0m\u001b[33m'\u001b[0m])/(df_infos2[\u001b[33m'\u001b[0m\u001b[33mprob_y\u001b[0m\u001b[33m'\u001b[0m]+df_infos2[\u001b[33m'\u001b[0m\u001b[33mprob_n\u001b[0m\u001b[33m'\u001b[0m]) \u001b[2m# Pro\u001b[0m     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0my = (df_infos2[\u001b[33m'\u001b[0m\u001b[33mans\u001b[0m\u001b[33m'\u001b[0m][:n//\u001b[94m2\u001b[0m] - df_infos2[\u001b[33m'\u001b[0m\u001b[33mans\u001b[0m\u001b[33m'\u001b[0m][n//\u001b[94m2\u001b[0m:].values).values>\u001b[94m0\u001b[0m \u001b[2m# Prob that righ\u001b[0m     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m4 \u001b[0mX = hss2[\u001b[94m0\u001b[0m][:n//\u001b[94m2\u001b[0m]-hss2[\u001b[94m0\u001b[0m][n//\u001b[94m2\u001b[0m:]                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/\u001b[0m\u001b[1;33mframe.py\u001b[0m:\u001b[94m3761\u001b[0m in       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92m__getitem__\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3758 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m is_single_key:                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3759 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.columns.nlevels > \u001b[94m1\u001b[0m:                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3760 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._getitem_multilevel(key)                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 3761 \u001b[2m         \u001b[0mindexer = \u001b[96mself\u001b[0m.columns.get_loc(key)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3762 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m is_integer(indexer):                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3763 \u001b[0m\u001b[2m            \u001b[0mindexer = [indexer]                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3764 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/home/ubuntu/mambaforge/envs/dlk2/lib/python3.9/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m3654\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mget_loc\u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3651 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3652 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3653 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3654 \u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3655 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mTypeError\u001b[0m:                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3656 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m# If we have a listlike key, _check_indexing_error will raise\u001b[0m                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3657 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'prob_y'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(df_infos2)\n",
    "df_infos2['ans'] = (df_infos2['prob_y'])/(df_infos2['prob_y']+df_infos2['prob_n']) # Prob of saying True\n",
    "y = (df_infos2['ans'][:n//2] - df_infos2['ans'][n//2:].values).values>0 # Prob that right one is more true\n",
    "X = hss2[0][:n//2]-hss2[0][n//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 # Try a regression</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 # split</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 4 n = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(y)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">'split size'</span>, n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>X_train, X_test = X[:n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>], X[n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:]                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>y_train, y_test = y[:n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>], y[n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:]                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m4\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[2m# Try a regression\u001b[0m                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 2 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m# split\u001b[0m                                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 4 n = \u001b[96mlen\u001b[0m(y)                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33msplit size\u001b[0m\u001b[33m'\u001b[0m, n//\u001b[94m2\u001b[0m)                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 6 \u001b[0mX_train, X_test = X[:n//\u001b[94m2\u001b[0m], X[n//\u001b[94m2\u001b[0m:]                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 7 \u001b[0my_train, y_test = y[:n//\u001b[94m2\u001b[0m], y[n//\u001b[94m2\u001b[0m:]                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'y'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try a regression\n",
    "\n",
    "# split\n",
    "n = len(y)\n",
    "print('split size', n//2)\n",
    "X_train, X_test = X[:n//2], X[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic regression accuracy: {:2.2f} [TRAIN]\".format(lr.score(X_train, y_train)))\n",
    "print(\"Logistic regression accuracy: {:2.2f} [TEST]\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>df_info_test = df_infos2.iloc[n//<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:].copy()                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2 y_pred = lr.predict(X_test)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>df_info_test[<span style=\"color: #808000; text-decoration-color: #808000\">'inner_truth'</span>] = y_pred                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>df_info_test                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1 \u001b[0mdf_info_test = df_infos2.iloc[n//\u001b[94m2\u001b[0m:].copy()                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2 y_pred = lr.predict(X_test)                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0mdf_info_test[\u001b[33m'\u001b[0m\u001b[33minner_truth\u001b[0m\u001b[33m'\u001b[0m] = y_pred                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m4 \u001b[0mdf_info_test                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'lr'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_info_test = df_infos2.iloc[n//2:].copy()\n",
    "y_pred = lr.predict(X_test)\n",
    "df_info_test['inner_truth'] = y_pred\n",
    "df_info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
