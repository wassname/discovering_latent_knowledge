{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A scratch pad to run model inference manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n",
    "from src.datasets.intervene import create_cache_interventions \n",
    "from src.prompts.prompt_loading import load_prompt_structure\n",
    "from src.repe import repe_pipeline_registry\n",
    "repe_pipeline_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config transformers\n",
    "from datasets import set_caching_enabled, disable_caching\n",
    "disable_caching()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # cache busting for the transformers map and ds steps\n",
    "# !rm -rf ~/.cache/huggingface/datasets/generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name='amazon_polarity'\n",
    "cfg = ExtractConfig(max_examples=(400, 400),\n",
    "                    intervention_fit_examples=160,\n",
    "                    )\n",
    "print(cfg)\n",
    "batch_size = cfg.batch_size\n",
    "\n",
    "model, tokenizer = load_model(cfg.model, pad_token_id=cfg.pad_token_id)\n",
    "print(model)\n",
    "\n",
    "N_train, N_test = cfg.max_examples\n",
    "N=sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_dataset(ds_name, tokenizer, N=N, seed=cfg.seed, num_shots=cfg.num_shots, max_length=cfg.max_length, prompt_format=cfg.prompt_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader = create_cache_interventions(model, tokenizer, cfg)\n",
    "honesty_rep_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "honesty_rep_reader.directions[layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_rep_reader.direction_signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.repe import repe_pipeline_registry\n",
    "from transformers import pipeline\n",
    "# from src.datasets.intervene import test_intervention_quality, intervention_metrics\n",
    "repe_pipeline_registry()\n",
    "\n",
    "honesty_rep_reader = create_cache_interventions(model, tokenizer, cfg)\n",
    "hidden_layers = sorted(honesty_rep_reader.directions.keys())\n",
    "coeff=1.\n",
    "\n",
    "activations = {}\n",
    "for layer in hidden_layers:\n",
    "    activations[layer] = torch.tensor(coeff * honesty_rep_reader.directions[layer] * honesty_rep_reader.direction_signs[layer]).to(model.device).half()\n",
    "assert torch.isfinite(torch.concat(list(activations.values()))).all()\n",
    "\n",
    "activations_neg_i = {k:-v for k,v in activations.items()}\n",
    "activations_neut = {k:v*0 for k,v in activations.items()}\n",
    "\n",
    "rep_control_pipeline2 = pipeline(\n",
    "    \"rep-control2\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    layers=hidden_layers)\n",
    "rep_control_pipeline2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_toke_probs(o, N=20):\n",
    "    data = {}\n",
    "    for i in range(o['end_logits'].shape[1]):\n",
    "        probs = torch.softmax(o['end_logits'][:, i], -1)\n",
    "        top = probs.argsort(0, descending=True)\n",
    "        top_probs = probs[top]\n",
    "        tokens_top20 = tokenizer.batch_decode(top[:N], skip_special_tokens=False , clean_up_tokenization_spaces=False)\n",
    "        tokens_top20 = [f\"`{t}`\" for t in tokens_top20] \n",
    "        data.update({f'prob_{i}':top_probs[:N], f'tokens_{i}':tokens_top20, f'id_{i}':top[:N]})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_pipeline_r(o):\n",
    "    choices = [tokenizer.batch_decode(cc) for cc in o['choice_ids']]\n",
    "    index = [o[0] for o in choices]\n",
    "    d = pd.DataFrame(o['choice_probs'].numpy(), columns=['edit=None', 'edit=+'], index=index)\n",
    "    print('choice probs')\n",
    "    display(d)\n",
    "\n",
    "    d1 = top_toke_probs(o)\n",
    "    print('top token probs')\n",
    "    display(d1)\n",
    "    top1 = o['end_logits'][:, 0].argsort(0, descending=True)[:10]\n",
    "    top2 = o['end_logits'][:, 1].argsort(0, descending=True)[:10]\n",
    "\n",
    "    max_prob1 = torch.softmax(o['end_logits'][:, 0], -1).max()\n",
    "    max_prob2 = torch.softmax(o['end_logits'][:, 1], -1).max()\n",
    "    print(top1)\n",
    "    print(top2)\n",
    "    print('top choices no intervention', tokenizer.batch_decode(top1, skip_special_tokens=False , clean_up_tokenization_spaces=False))\n",
    "    print('top choices pos intervention', tokenizer.batch_decode(top2))\n",
    "    \n",
    "    mean_prob = o['choice_probs'].sum(0)\n",
    "    print(f\"\\tchoice_cov=\\t{mean_prob[0]:2.2%} max_prob={max_prob1} (no edit) - Our choices accounted for a mean probability of this\")\n",
    "    print(f\"\\tchoice_cov=\\t{mean_prob[1]:2.2%} max_prob={max_prob2} (+ edit) - Our choices accounted for a mean probability of this\")\n",
    "    \n",
    "    print('choices', choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_tokens.select(range(3)).to_iterable_dataset()\n",
    "r1 = rep_control_pipeline2(model_inputs=ds,\n",
    "        activations=activations_neg_i,\n",
    "        batch_size=batch_size,)\n",
    "r = list(r1)\n",
    "o = r[0]\n",
    "print_pipeline_r(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_tokens.select(range(3)).to_iterable_dataset()\n",
    "r1 = rep_control_pipeline2(model_inputs=ds,\n",
    "        activations=activations,\n",
    "        batch_size=batch_size,)\n",
    "r = list(r1)\n",
    "print_pipeline_r(r[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch choice ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.scores import choice2id\n",
    "\n",
    "choice2id(tokenizer, 'Positive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate long form with and without intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = 20\n",
    "inputs = ds_tokens.with_format('torch')[bi]\n",
    "\n",
    "# tokenize if needed\n",
    "if 'input_ids' not in inputs:\n",
    "    model_inputs = self.tokenizer(inputs['question'], return_tensors=True, return_attention_mask=True, add_special_tokens=True, truncation=True, padding=\"max_length\", max_length=cfg.max_length, **tokenize_kwargs)\n",
    "    inputs = {**inputs, **model_inputs}\n",
    "\n",
    "inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "# model = exllama_set_max_input_length(model, 4096)\n",
    "@torch.no_grad()\n",
    "def gen(model):\n",
    "    s = model.generate(inputs['input_ids'][None, :], attention_mask=inputs['attention_mask'][None, :], use_cache=False, max_new_tokens=20, min_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "    input_l = inputs['input_ids'].shape[0]\n",
    "    old = tokenizer.decode(s[0, :input_l], clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "    new = tokenizer.decode(s[0, input_l:], clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "    display(HTML(f\"<pre>{old}</pre><b><pre>{new}</pre></b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.nethook import Trace, TraceDict, recursive_copy\n",
    "from functools import partial\n",
    "from src.repe.rep_control_pipeline_baukit import intervention_meta_fn2, Activations\n",
    "layers_names = [rep_control_pipeline2.layer_name_tmpl.format(i) for i in activations.keys()]   \n",
    "activations_pos_i = Activations({rep_control_pipeline2.layer_name_tmpl.format(k):1. * v for k,v in activations.items()})\n",
    "edit_fn_pos = partial(intervention_meta_fn2, activations=activations_pos_i)\n",
    "with torch.no_grad():\n",
    "    with TraceDict(\n",
    "        model, layers_names, detach=True, edit_output=edit_fn_pos\n",
    "    ) as ret:\n",
    "        gen(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
