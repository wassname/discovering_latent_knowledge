{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets save our data as a huggingface dataset, so it's quick to reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:39.840442Z",
     "start_time": "2023-09-02T11:00:38.221653Z"
    }
   },
   "outputs": [],
   "source": [
    "# import your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from loguru import logger\n",
    "import sys\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"<level>{message}</level>\", level=\"INFO\")\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:42.996618Z",
     "start_time": "2023-09-02T11:00:39.841585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetInfo, load_from_disk, load_dataset, IterableDataset\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, re, sys, collections, functools, itertools, json\n",
    "\n",
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.258472Z",
     "start_time": "2023-09-02T11:00:43.000477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ubuntu/mambaforge/envs/dlk3/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ubuntu/mambaforge/envs/dlk3/lib/libcudart.so'), PosixPath('/home/ubuntu/mambaforge/envs/dlk3/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from src.models.load import load_model\n",
    "from src.datasets.load import ds2df\n",
    "from src.datasets.load import rows_item\n",
    "from src.datasets.batch import batch_hidden_states\n",
    "# from src.datasets.scores import choice2ids, scores2choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.316850Z",
     "start_time": "2023-09-02T11:00:46.259480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractConfig(model='WizardLM/WizardCoder-3B-V1.0', datasets=('qasc',), data_dirs=(), int4=True, max_examples=(250, 31), num_shots=1, num_variants=-1, layers=(), seed=42, token_loc='last', template_path=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "BATCH_SIZE = 1  # None # None means auto # 6 gives 16Gb/25GB. where 10GB is the base model. so 6 is 6/15\n",
    "USE_MCDROPOUT = True\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    # model=\"HuggingFaceH4/starchat-beta\",\n",
    "    # model=\"TheBloke/CodeLlama-13B-Instruct-fp16\", # too large!\n",
    "    model=\"WizardLM/WizardCoder-3B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-1B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-Python-7B-V1.0\", # too large!\n",
    "    \n",
    "    ## see https://github.com/EleutherAI/elk/tree/1b60b3bff348b00356cd15b5eb017f9c9bfdbae1/elk/promptsource/templates\n",
    "    datasets = (\n",
    "        # \"imdb\", # sentiment\n",
    "        # \"amazon_polarity\", # sentiment\n",
    "        # \"super_glue:boolq\", # reading comprehension\n",
    "        # 'tweet_eval:irony', # irony\n",
    "        # 'great_code', # code\n",
    "        'qasc', # Question Answering via Sentence Composition (QASC) # dataset has no label column\n",
    "        \n",
    "        ## Datasets with problems\n",
    "        # 'lauritowal/redefine_math', # dataset has no label column\n",
    "        # 'crows_pairs', # sterotypes FAIL need to specify label columns\n",
    "        # 'hate_speech18', # weird errors\n",
    "        # 'medical_questions_pairs', # medical paraphrase        \n",
    "        # 'poem_sentiment' # no only boolean for now\n",
    "        # 'reaganjlee/truthful_qa_mc', # no only bool\n",
    "    ),\n",
    "    max_examples=(250, 31),\n",
    "    num_shots=1,\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Chosing:\n",
    "- https://old.reddit.com/r/LocalLLaMA/wiki/models\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- https://github.com/deep-diver/LLM-As-Chatbot/blob/main/model_cards.json\n",
    "\n",
    "\n",
    "A uncensored and large coding ones might be best for lying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:50.889443Z",
     "start_time": "2023-09-02T11:00:46.318029Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models.load import verbose_change_param, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_repo = \"HuggingFaceH4/starchat-beta\"):\n",
    "    # see https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/starchat.py\n",
    "    model_options = dict(\n",
    "        device_map=\"auto\",\n",
    "        # load_in_8bit=True,\n",
    "        # load_in_4bit=True,\n",
    "        torch_dtype=torch.float16, # note because datasets pickles the model into numpy to get the unique datasets name, and because numpy doesn't support bfloat16, we need to use float16\n",
    "        # use_safetensors=False,\n",
    "    )\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_repo, use_cache=False)\n",
    "    verbose_change_param(config, 'use_cache', False)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "    verbose_change_param(tokenizer, 'pad_token_id', 0)\n",
    "    verbose_change_param(tokenizer, 'padding_side', 'left')\n",
    "    verbose_change_param(tokenizer, 'truncation_side', 'left')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_repo, config=config, **model_options)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/imdb ../src/prompts/templates/imdb\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/amazon_polarity ../src/prompts/templates/amazon_polarity\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/super_glue ../src/prompts/templates/super_glue\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/tweet_eval ../src/prompts/templates/tweet_eval\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/great_code ../src/prompts/templates/great_code\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/qasc ../src/prompts/templates/qasc\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/lauritowal/redefine_math ../src/prompts/templates/lauritowal/redefine_math\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/crows_pairs ../src/prompts/templates/crows_pairs\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/hate_speech18 ../src/prompts/templates/hate_speech18\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/medical_questions_pairs ../src/prompts/templates/medical_questions_pairs\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/poem_sentiment ../src/prompts/templates/poem_sentiment\n",
      "/home/ubuntu/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/promptsource/templates/reaganjlee/truthful_qa_mc ../src/prompts/templates/reaganjlee/truthful_qa_mc\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525457Z",
     "start_time": "2023-09-02T11:02:54.525448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afac224ac9476a816b5f08b2aeb91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633ba466d56149e5aaa31f7f3da642dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154ca74fe62c4950b1bf94325d6e12bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd9911595644c479786c795fe7c80c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94601f77bbe44681abec3a61066ed479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf34a0e44274368b563e4225ea4948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65eb9f3282d24779b0be502d4263033d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b952d30796446bd92f7704c2b7ed47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 8 variants of each prompt\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:1676\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1675\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m-> 1676\u001b[0m \u001b[39mfor\u001b[39;00m key, record \u001b[39min\u001b[39;00m generator:\n\u001b[1;32m   1677\u001b[0m     \u001b[39mif\u001b[39;00m max_shard_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m writer\u001b[39m.\u001b[39m_num_bytes \u001b[39m>\u001b[39m max_shard_size:\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/packaged_modules/generator/generator.py:30\u001b[0m, in \u001b[0;36mGenerator._generate_examples\u001b[0;34m(self, **gen_kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_examples\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mfor\u001b[39;00m idx, ex \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mgenerator(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs)):\n\u001b[1;32m     31\u001b[0m         \u001b[39myield\u001b[39;00m idx, ex\n",
      "File \u001b[0;32m~/Documents/mjc/elk/discovering_latent_knowledge/src/prompts/prompt_loading.py:122\u001b[0m, in \u001b[0;36mload_prompts\u001b[0;34m(ds_string, sys_instructions, binarize, num_shots, seed, split_type, template_path, rank, world_size, prompt_format, prompt_sampler, N)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00mnum_templates\u001b[39m}\u001b[39;00m\u001b[39m variants of each prompt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m label_column \u001b[39m=\u001b[39m prompter\u001b[39m.\u001b[39mlabel_column \u001b[39mor\u001b[39;00m infer_label_column(ds\u001b[39m.\u001b[39;49mfeatures)\n\u001b[1;32m    124\u001b[0m label_feature \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mfeatures[label_column]\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/elk/utils/data_utils.py:93\u001b[0m, in \u001b[0;36minfer_label_column\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m label_cols:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset has no label column\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(label_cols) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset has no label column",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m ds_name \u001b[39m=\u001b[39m ds_names[\u001b[39m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m N \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mmax_examples[split_type\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_generator(\n\u001b[1;32m     12\u001b[0m     load_prompts, \n\u001b[1;32m     13\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[1;32m     14\u001b[0m         ds_string\u001b[39m=\u001b[39;49mds_name, \n\u001b[1;32m     15\u001b[0m         num_shots\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mnum_shots,\n\u001b[1;32m     16\u001b[0m         split_type\u001b[39m=\u001b[39;49msplit_type,\n\u001b[1;32m     17\u001b[0m         template_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mtemplate_path,\n\u001b[1;32m     18\u001b[0m         seed\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m     19\u001b[0m         prompt_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mllama\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m         N\u001b[39m=\u001b[39;49mN,\n\u001b[1;32m     21\u001b[0m     ), \n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m dataset\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/arrow_dataset.py:1072\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \n\u001b[1;32m   1018\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerator\u001b[39;00m \u001b[39mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1064\u001b[0m \u001b[39mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[1;32m   1065\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m   1066\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1067\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1068\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1069\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m   1070\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1071\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m-> 1072\u001b[0m )\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/io/generator.py:47\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     verification_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     base_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m     48\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m     49\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m     50\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m     51\u001b[0m         \u001b[39m# try_from_hf_gcs=try_from_hf_gcs,\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m         base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m     53\u001b[0m         num_proc\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mas_dataset(\n\u001b[1;32m     56\u001b[0m         split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, verification_mode\u001b[39m=\u001b[39mverification_mode, in_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_in_memory\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 954\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    957\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    958\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1718\u001b[0m         dl_manager,\n\u001b[1;32m   1719\u001b[0m         verification_mode,\n\u001b[1;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1721\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1722\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1723\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:1049\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m   1047\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1052\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1053\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m   1056\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:1555\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1553\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1554\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1556\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1557\u001b[0m     ):\n\u001b[1;32m   1558\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   1559\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/datasets/builder.py:1712\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1711\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[0;32m-> 1712\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import functools\n",
    "from src.prompts.prompt_loading import load_prompts\n",
    "        \n",
    "# loop through all prompts in this dataset\n",
    "ds_names = cfg.datasets\n",
    "split_type = \"train\"\n",
    "\n",
    "ds_name = ds_names[0]\n",
    "N = cfg.max_examples[split_type!=\"train\"]\n",
    "dataset = Dataset.from_generator(\n",
    "    load_prompts, \n",
    "    gen_kwargs=dict(\n",
    "        ds_string=ds_name, \n",
    "        num_shots=cfg.num_shots,\n",
    "        split_type=split_type,\n",
    "        template_path=cfg.template_path,\n",
    "        seed=cfg.seed,\n",
    "        prompt_format='llama',\n",
    "        N=N,\n",
    "    ), \n",
    "    )\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525970Z",
     "start_time": "2023-09-02T11:02:54.525961Z"
    }
   },
   "outputs": [],
   "source": [
    "b = next(iter(dataset))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format prompts\n",
    "\n",
    "The prompt is the thing we most often have to change and debug. So we do it explicitly here.\n",
    "\n",
    "We do it as transforms on a huggingface dataset.\n",
    "\n",
    "In this case we use multishot examples from train, and use the test set to generated the hidden states dataset. We will test generalisation on a whole new dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.scores import scores2choice_probs\n",
    "from src.datasets.scores import choice2id, choice2ids\n",
    "\n",
    "def row_choice_ids(r):\n",
    "    return choice2ids([[c] for c in r['answer_choices']], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.526826Z",
     "start_time": "2023-09-02T11:02:54.526815Z"
    },
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = (\n",
    "    dataset\n",
    "    .map(\n",
    "        lambda ex: tokenizer(\n",
    "            ex[\"question\"], padding=\"max_length\", max_length=600, truncation=True, add_special_tokens=True,\n",
    "            # return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(\n",
    "        lambda r: {\"prompt_truncated\": tokenizer.batch_decode(r[\"input_ids\"])},\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(lambda r: {'choice_ids': row_choice_ids(r)})\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.527638Z",
     "start_time": "2023-09-02T11:02:54.527629Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dataset filename\n",
    "sanitize = lambda s:s.replace('/', '').replace('-', '_') if s is not None else s\n",
    "\n",
    "dataset_name = f\"{sanitize(cfg.model)}_{ds_name}_{split_type}_{N}\"\n",
    "dataset_name\n",
    "\n",
    "f = f\"../.ds/{dataset_name}\"\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.528377Z",
     "start_time": "2023-09-02T11:02:54.528364Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.528958Z",
     "start_time": "2023-09-02T11:02:54.528949Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_kwargs = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds['choice_ids']\n",
    "l = model.transformer.h[10]\n",
    "l.attn.c_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.529566Z",
     "start_time": "2023-09-02T11:02:54.529557Z"
    }
   },
   "outputs": [],
   "source": [
    "info_kwargs = dict(extract_cfg=cfg, ds_name=ds_name, split_type=split_type, f=f)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test, debug\n",
    "# g = batch_hidden_states(**gen_kwargs)\n",
    "# bb = next(iter(g))\n",
    "# print({k:bb[k].shape for k in bb['large_arrays_keys']})\n",
    "# print({k:bb[k].dtype for k in bb['large_arrays_keys']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.helpers.typing import float_to_int16, int16_to_float\n",
    "# import torch\n",
    "# x = torch.rand(4, 5, dtype=torch.float)\n",
    "# x2 = float_to_int16(x)\n",
    "# x3 = int16_to_float(x2)\n",
    "# x3-x\n",
    "# # x.type(torch.float)-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DatasetInfo](https://github.com/huggingface/datasets/blob/9b21e181b642bd55b3ef68c1948bfbcd388136d6/src/datasets/info.py#L94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.529966Z",
     "start_time": "2023-09-02T11:02:54.529959Z"
    }
   },
   "outputs": [],
   "source": [
    "ds1 = Dataset.from_generator(\n",
    "    generator=batch_hidden_states,\n",
    "    info=DatasetInfo(\n",
    "        name=dataset_name,\n",
    "        description=json.dumps(info_kwargs, indent=2),)\",\n",
    "        config_name=f,\n",
    "        citation=\"\",\n",
    "        homepage=\"\",\n",
    "        version=\"\",\n",
    "        \n",
    "        \n",
    "    ),\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    num_proc=1,\n",
    "    \n",
    ")#.with_format(\"numpy\")\n",
    "# ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Add labels\n",
    "\n",
    "For our probe. Given next_token scores (logits) we take only the subset the corresponds to our negative tokens (e.g. False, no, ...) and positive tokens (e.g. Yes, yes, affirmative, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.530402Z",
     "start_time": "2023-09-02T11:02:54.530395Z"
    }
   },
   "outputs": [],
   "source": [
    "# from src.datasets.scores import choice2id, choice2ids\n",
    "# ds1['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.530973Z",
     "start_time": "2023-09-02T11:02:54.530965Z"
    }
   },
   "outputs": [],
   "source": [
    "# def expand_choices(choices: List[str]) -> List[str]:\n",
    "#     \"\"\"expand out choices by adding versions that are upper, lower, whitespace, etc\"\"\"\n",
    "#     new = []\n",
    "#     for c in choices:\n",
    "#         new.append(c)\n",
    "#         new.append(c.upper())\n",
    "#         new.append(c.capitalize())\n",
    "#         new.append(c.lower())\n",
    "#     return set(new)\n",
    "\n",
    "\n",
    "# left_choices = list(r[0] for r in ds1['answer_choices'])+['no', 'false', 'negative', 'wrong']\n",
    "# right_choices = list(r[1] for r in ds1['answer_choices'])+['yes', 'true', 'positive', 'right']\n",
    "# left_choices, right_choices = expand_choices(left_choices), expand_choices(right_choices)\n",
    "# expanded_choices = [left_choices, right_choices]\n",
    "# expanded_choice_ids = choice2ids(expanded_choices, tokenizer)\n",
    "# expanded_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.531369Z",
     "start_time": "2023-09-02T11:02:54.531361Z"
    }
   },
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.532062Z",
     "start_time": "2023-09-02T11:02:54.532054Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is just based on pairs for that answer...\n",
    "add_txt_ans0 = lambda r: {'txt_ans0': tokenizer.decode(r['scores0'].argmax(-1))}\n",
    "# add_txt_ans1 = lambda r: {'txt_ans1': tokenizer.decode(r['scores1'].argmax(-1))}\n",
    "\n",
    "def row_choice_ids(r):\n",
    "    return choice2ids([[c] for c in r['answer_choices']], tokenizer)\n",
    "\n",
    "# Either just use the template choices\n",
    "add_ans = lambda r: scores2choice_probs(r, row_choice_ids(r), keys=[\"scores0\"])\n",
    "\n",
    "# Or all expanded choices\n",
    "# add_ans_exp = lambda r: scores2choice_probs(r, expanded_choice_ids, prefix=\"expanded_\")\n",
    "ds1.set_format(type='numpy')#, columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "ds3 = (\n",
    "    ds1\n",
    "    .map(add_ans)\n",
    "    # .map(add_ans_exp)\n",
    "    .map(add_txt_ans0)\n",
    "    # .map(add_txt_ans1)\n",
    ")\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3.config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.532589Z",
     "start_time": "2023-09-02T11:02:54.532582Z"
    }
   },
   "outputs": [],
   "source": [
    "ds3.save_to_disk(f)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.533085Z",
     "start_time": "2023-09-02T11:02:54.533078Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets.load import load_ds\n",
    "ds4 = load_ds(f)\n",
    "ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [v for k,v in ds4[0].items() if isinstance(v, (np.ndarray, np.generic, torch.Tensor))]\n",
    "for k,v in ds4[0].items():\n",
    "    print(k, v.shape, v.dtype)\n",
    "    if (isinstance(v, (np.ndarray, np.generic, torch.Tensor)) and (v.dtype in ['float16', 'float32', 'float64', 'int64', 'int32', 'int16', 'int8'])):\n",
    "        assert np.isfinite(v).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.533535Z",
     "start_time": "2023-09-02T11:02:54.533528Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# QC, check which answers are most common\n",
    "common_answers = pd.Series(ds4['txt_ans0']).value_counts()\n",
    "display('Remember it should be binary. Found common LLM answers:', common_answers)\n",
    "\n",
    "current_choices = set(list(chain(*ds4['answer_choices'])))\n",
    "unexpected_answers = set(common_answers.head(10).index)-current_choices\n",
    "if len(unexpected_answers):\n",
    "    logger.warning(f'found unexpected answers: {unexpected_answers}. You may want to add them to class2choices')\n",
    "    \n",
    "mean_prob = ds4['choice_probs0'].sum(-1).mean()\n",
    "print('mean_prob', mean_prob)\n",
    "assert ds4['choice_probs0'].sum(-1).mean()>0.2, f\"\"\"\n",
    "Our choices should cover most common answers. But they accounted for a mean probability of {mean_prob:2.2%} (should be >40%). \n",
    "\n",
    "To fix this you might want to improve your prompt or add to your choices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534012Z",
     "start_time": "2023-09-02T11:02:54.534004Z"
    }
   },
   "outputs": [],
   "source": [
    "df = ds2df(ds4)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534378Z",
     "start_time": "2023-09-02T11:02:54.534370Z"
    }
   },
   "outputs": [],
   "source": [
    "# QC check accuracy\n",
    "# it should manage to lie some of the time when asked to lie. Many models wont lie unless very explicitly asked to, but we don't want to do that, we want to leave some ambiguity in the prompt\n",
    "\n",
    "d = df.query('instructed_to_lie==True')\n",
    "acc = (d.label_instructed==d.llm_ans).mean()\n",
    "print(f\"when the model tries to lie... we get this acc {acc:2.2f}\")\n",
    "assert acc>0.1, f\"should be acc>0.1 but is acc={acc}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534845Z",
     "start_time": "2023-09-02T11:02:54.534837Z"
    }
   },
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    return dict(\n",
    "        acc=(df.llm_ans == df.label_instructed).mean(),\n",
    "        n=len(df),\n",
    "    )\n",
    "    \n",
    "def col2statsdf(df, group):\n",
    "    return pd.DataFrame(df.groupby(group).apply(stats).to_dict()).T\n",
    "    \n",
    "    \n",
    "print(\"how well does it do the simple task of telling the truth, for each template\")\n",
    "col2statsdf(df.query('sys_instr_name==\"truth\"'), 'template_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.535378Z",
     "start_time": "2023-09-02T11:02:54.535370Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"how well does it complete the task for each prompt\")\n",
    "# of course getting it to tell the truth is easy, but how effective are the other prompts?\n",
    "col2statsdf(df, 'sys_instr_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC view row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.535946Z",
     "start_time": "2023-09-02T11:02:54.535938Z"
    }
   },
   "outputs": [],
   "source": [
    "# QC by viewing a row\n",
    "r = ds4[0]\n",
    "print(r['prompt_truncated'])\n",
    "print(r['txt_ans0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC: generation\n",
    "\n",
    "Let's a quick generation, so we can QC the output and sanity check that the model can actually do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.536320Z",
     "start_time": "2023-09-02T11:02:54.536313Z"
    }
   },
   "outputs": [],
   "source": [
    "# r = ds[2]\n",
    "# q = r[\"prompt_truncated\"]\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# sequences = pipeline(\n",
    "#     q.lstrip('<|endoftext|>'),\n",
    "##     max_length=100,\n",
    "# max_new_tokens=10,\n",
    "#     do_sample=False,\n",
    "#     return_full_text=False,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(\"-\" * 80)\n",
    "#     print(q)\n",
    "#     print(\"-\" * 80)\n",
    "#     print(f\"`{seq['generated_text']}`\")\n",
    "#     print(\"-\" * 80)\n",
    "#     print(\"label\", r['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# QC: linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.536825Z",
     "start_time": "2023-09-02T11:02:54.536818Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just select the question where the model knows the answer. \n",
    "df = ds2df(ds4)\n",
    "d = df.query('sys_instr_name==\"truth\"').set_index(\"example_i\")\n",
    "\n",
    "# # these are the ones where it got it right when asked to tell the truth\n",
    "m1 = d.llm_ans==d.label_true\n",
    "known_indices = d[m1].index\n",
    "print(f\"select rows are {m1.mean():2.2%} based on knowledge\")\n",
    "# # convert to row numbers, and use datasets to select\n",
    "known_rows = df['example_i'].isin(known_indices)\n",
    "known_rows_i = df[known_rows].index\n",
    "\n",
    "# # also restrict it to significant permutations. That is monte carlo dropout pairs, where the answer changes by more than X%\n",
    "# m = np.abs(df.ans0-df.ans1)>0.05\n",
    "# print(f\"selected rows are {m.mean():2.2%} for significance\")\n",
    "# significant_rows = m[m].index\n",
    "\n",
    "# allowed_rows_i = set(known_rows_i).intersection(significant_rows)\n",
    "# allowed_rows_i = significant_rows\n",
    "ds5 = ds4.select(known_rows_i)\n",
    "df = ds2df(ds5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [v for k,v in ds4[0].items()]\n",
    "# ds4[0]['hidden_states'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_arrays_keys = [k for k,v in ds4[0].items() if v.ndim>1]\n",
    "large_arrays_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.537283Z",
     "start_time": "2023-09-02T11:02:54.537276Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in large_arrays_keys:\n",
    "    print('-'*80)\n",
    "    print(k)\n",
    "    hs = ds5[k]\n",
    "    X = hs.reshape(hs.shape[0], -1)\n",
    "\n",
    "\n",
    "    y = df['label_true'] == df['llm_ans']\n",
    "\n",
    "    # split\n",
    "    n = len(y)\n",
    "    max_rows = 1000\n",
    "    \n",
    "    X_train, X_test = X[:n//2], X[n//2:]\n",
    "    y_train, y_test = y[:n//2], y[n//2:]\n",
    "    X_train = X_train[:max_rows]\n",
    "    y_train = y_train[:max_rows]\n",
    "    X_test = X_test[:max_rows]\n",
    "    y_test = y_test[:max_rows]\n",
    "    print('split size', X_train.shape, y_test.shape)\n",
    "\n",
    "    # scale\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train2 = scaler.transform(X_train)\n",
    "    X_test2 = scaler.transform(X_test)\n",
    "\n",
    "    lr = LogisticRegression(class_weight=\"balanced\", penalty=\"l2\", max_iter=380)\n",
    "    lr.fit(X_train2, y_train>0)\n",
    "\n",
    "    print(\"Logistic cls acc: {: 3.2%} [TRAIN]\".format(lr.score(X_train2, y_train>0)))\n",
    "    print(\"Logistic cls acc: {: 3.2%} [TEST]\".format(lr.score(X_test2, y_test>0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC: make sure we didn't lose all of the successful lies, which would make the problem trivial\n",
    "df2= ds2df(ds5)\n",
    "df_subset_successull_lies = df2.query(\"instructed_to_lie==True & (llm_ans==label_instructed)\")\n",
    "print(f\"filtered to {len(df_subset_successull_lies)} num successful lies out of {len(df2)} dataset rows\")\n",
    "assert len(df_subset_successull_lies)>0, \"there should be successful lies in the dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
