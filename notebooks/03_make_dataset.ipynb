{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets save our data as a huggingface dataset, so it's quick to reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:39.840442Z",
     "start_time": "2023-09-02T11:00:38.221653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from loguru import logger\n",
    "import sys\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"<level>{message}</level>\", level=\"INFO\")\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:42.996618Z",
     "start_time": "2023-09-02T11:00:39.841585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetInfo, load_from_disk, load_dataset\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, re, sys, collections, functools, itertools, json\n",
    "\n",
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.258472Z",
     "start_time": "2023-09-02T11:00:43.000477Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models.load import load_model\n",
    "from src.datasets.load import ds2df\n",
    "from src.datasets.load import rows_item\n",
    "from src.datasets.batch import batch_hidden_states\n",
    "# from src.datasets.scores import choice2ids, scores2choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:00:46.316850Z",
     "start_time": "2023-09-02T11:00:46.259480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractConfig(model='WizardLM/WizardCoder-3B-V1.0', datasets=['imdb'], data_dirs=(), int4=True, max_examples=(12, 12), num_shots=2, num_variants=-1, layers=(), seed=42, token_loc='last', template_path=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "BATCH_SIZE = 1  # None # None means auto # 6 gives 16Gb/25GB. where 10GB is the base model. so 6 is 6/15\n",
    "USE_MCDROPOUT = True\n",
    "\n",
    "from src.extraction.config import ExtractConfig\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    # model=\"HuggingFaceH4/starchat-beta\",\n",
    "    # model=\"TheBloke/CodeLlama-13B-Instruct-fp16\", # too large!\n",
    "    model=\"WizardLM/WizardCoder-3B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-1B-V1.0\",\n",
    "    # model=\"WizardLM/WizardCoder-Python-7B-V1.0\", # too large!\n",
    "    datasets = [\n",
    "        \"imdb\", \n",
    "        # \"amazon_polarity\",\n",
    "        # \"truthful_qa\",\n",
    "                #\"super_glue:boolq\", \"EleutherAI/truthful_qa_mc\", \"EleutherAI/arithmetic\", \"NeelNanda/counterfact-tracing\"\n",
    "                ],\n",
    "    max_examples=(12, 12),\n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Chosing:\n",
    "- https://old.reddit.com/r/LocalLLaMA/wiki/models\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- https://github.com/deep-diver/LLM-As-Chatbot/blob/main/model_cards.json\n",
    "\n",
    "\n",
    "A uncensored and large coding ones might be best for lying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:50.889443Z",
     "start_time": "2023-09-02T11:00:46.318029Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_repo, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_options)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m model, tokenizer\n\u001b[0;32m---> 25\u001b[0m model, tokenizer \u001b[39m=\u001b[39m load_model(cfg\u001b[39m.\u001b[39;49mmodel)\n",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_repo)\u001b[0m\n\u001b[1;32m     13\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_repo, use_cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m verbose_change_param(config, \u001b[39m'\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_repo)\n\u001b[1;32m     17\u001b[0m verbose_change_param(tokenizer, \u001b[39m'\u001b[39m\u001b[39mpad_token_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m verbose_change_param(tokenizer, \u001b[39m'\u001b[39m\u001b[39mpadding_side\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:652\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    654\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:496\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 496\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    497\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    498\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    499\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    500\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    501\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    502\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    503\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    504\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    505\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    506\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    507\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    508\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    509\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    510\u001b[0m )\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(Timeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/mambaforge/envs/dlk3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.models.load import verbose_change_param, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_repo = \"HuggingFaceH4/starchat-beta\"):\n",
    "    # see https://github.com/deep-diver/LLM-As-Chatbot/blob/main/models/starchat.py\n",
    "    model_options = dict(\n",
    "        device_map=\"auto\",\n",
    "        # load_in_8bit=True,\n",
    "        # load_in_4bit=True,\n",
    "        torch_dtype=torch.float16, # note because datasets pickles the model into numpy to get the unique datasets name, and because numpy doesn't support bfloat16, we need to use float16\n",
    "        # use_safetensors=False,\n",
    "    )\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_repo, use_cache=False)\n",
    "    verbose_change_param(config, 'use_cache', False)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "    verbose_change_param(tokenizer, 'pad_token_id', 0)\n",
    "    verbose_change_param(tokenizer, 'padding_side', 'left')\n",
    "    verbose_change_param(tokenizer, 'truncation_side', 'left')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_repo, config=config, **model_options)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_y = tokenizer(' True').input_ids\n",
    "token_n = tokenizer(' Fakse').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.228482Z",
     "start_time": "2023-09-02T11:02:50.891428Z"
    }
   },
   "outputs": [],
   "source": [
    "# from src.datasets.dropout import enable_dropout\n",
    "\n",
    "# def get_gradients(model, outputs,token_y, token_n):\n",
    "#     model.zero_grad()\n",
    "#     score_y = outputs[\"scores\"][:, token_y]\n",
    "#     score_n = outputs[\"scores\"][:, token_n]\n",
    "#     pred = score_y - score_n\n",
    "#     loss = F.mse_loss(pred, -pred)\n",
    "#     loss.backward()\n",
    "#     ps = model.named_parameters()\n",
    "#     grads = {n:g.grad.cpu() for n,g in ps if g.grad is not None}\n",
    "#     model.zero_grad()\n",
    "#     model.eval()\n",
    "#     return grads\n",
    "\n",
    "    \n",
    "# input_text =\"the sky is blue is a statement which is \"\n",
    "# truncation_length = 999\n",
    "# t = tokenizer(\n",
    "#     input_text,\n",
    "#     return_tensors=\"pt\",\n",
    "#     add_special_tokens=True,\n",
    "#     padding='max_length', max_length=truncation_length, truncation=True, return_attention_mask=True,\n",
    "# )\n",
    "# input_ids = t.input_ids.to(model.device)\n",
    "# attention_mask = t.attention_mask.to(model.device)\n",
    "\n",
    "# model_inputs = model.prepare_inputs_for_generation(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)\n",
    "# outputs = model.forward(\n",
    "#     **model_inputs,\n",
    "#     return_dict=True,\n",
    "#     output_hidden_states=True,\n",
    "# )\n",
    "# outputs[\"scores\"] = outputs.logits[:, -1, :]\n",
    "\n",
    "# grads_all = get_gradients(model, outputs,token_y, token_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[\"scores\"].shape\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs['hidden_states'][0][0, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# p = \".+mlp.c_proj.weight\" # get the last weight of each layer (ignore bias)\n",
    "# grads = torch.stack([g.mean(1).float() for k,g in grads_all.items() if re.match(p, k)]).numpy()\n",
    "# grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = \".+mlp.c_proj.bias\" # get the last weight of each layer (ignore bias)\n",
    "# grads = torch.stack([g.float() for k,g in grads_all.items() if re.match(p, k)]).numpy()\n",
    "# grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(grads.flatten(), 55, density=True)\n",
    "# plt.title('grads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hs = outputs['hidden_states'][5][0, -1].cpu().float().detach()\n",
    "# plt.hist(hs, 55, density=True)\n",
    "# plt.title('hidden_states')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525457Z",
     "start_time": "2023-09-02T11:02:54.525448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067679e7926c4ea1abc0fb60ea06e403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 13 variants of each prompt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'question', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name'],\n",
       "    num_rows: 12\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain, islice\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "# from datasets.arrow_dataset import Dataset\n",
    "from src.prompts.prompt_loading import load_prompts\n",
    "\n",
    "@functools.lru_cache()\n",
    "def count_tokens(s):\n",
    "    return len(tokenizer(s).input_ids)\n",
    "\n",
    "def answer_len(answer_choices: list):\n",
    "    a = count_tokens(answer_choices[0])\n",
    "    b = count_tokens(answer_choices[1])\n",
    "    return max(a, b)\n",
    "\n",
    "\n",
    "def sample_n_true_y_false_prompts(prompts, num_truth=1, num_lie=1, seed=42):\n",
    "    \"\"\"sample some truth and some false\"\"\"\n",
    "    df = pd.DataFrame(prompts)\n",
    "    \n",
    "    # restrict to template where the choices are a single token\n",
    "    m = df.answer_choices.map(answer_len)<=2\n",
    "    df = df[m]\n",
    "    df = pd.concat([\n",
    "        df.query(\"instructed_to_lie==True\").sample(num_truth, random_state=seed),\n",
    "        df.query(\"instructed_to_lie==False\").sample(num_lie, random_state=seed)])\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# for ds_name in ds_names:\n",
    "#     for split_type in [\"train\", \"test\"]:\n",
    "        \n",
    "# loop through all prompts in this dataset\n",
    "ds_names = cfg.datasets\n",
    "split_type = \"train\"\n",
    "\n",
    "ds_name = ds_names[0]\n",
    "prompt_ds = load_prompts(\n",
    "    ds_name,\n",
    "    num_shots=cfg.num_shots,\n",
    "    split_type=split_type,\n",
    "    template_path=cfg.template_path,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# for each example, sample true and false\n",
    "N = cfg.max_examples[split_type!=\"train\"]\n",
    "g = map(lambda r: sample_n_true_y_false_prompts(r[1], seed=r[0]+cfg.seed), enumerate(prompt_ds))\n",
    "\n",
    "# and combine them into one big list\n",
    "g = chain.from_iterable(g) \n",
    "prompt_ds2 = list(tqdm(islice(g, N), total=N))\n",
    "\n",
    "# convert to hugginface dataset\n",
    "dataset = Dataset.from_list(prompt_ds2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.525970Z",
     "start_time": "2023-09-02T11:02:54.525961Z"
    }
   },
   "outputs": [],
   "source": [
    "# b = next(iter(prompt_ds))\n",
    "# b\n",
    "# sample_n_true_y_false_prompts(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add choice tokens to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format prompts\n",
    "\n",
    "The prompt is the thing we most often have to change and debug. So we do it explicitly here.\n",
    "\n",
    "We do it as transforms on a huggingface dataset.\n",
    "\n",
    "In this case we use multishot examples from train, and use the test set to generated the hidden states dataset. We will test generalisation on a whole new dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.scores import scores2choice_probs\n",
    "from src.datasets.scores import choice2id, choice2ids\n",
    "\n",
    "def row_choice_ids(r):\n",
    "    return choice2ids([[c] for c in r['answer_choices']], tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.526826Z",
     "start_time": "2023-09-02T11:02:54.526815Z"
    },
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238c3d4522cc41ae982be84f23dc1c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74621fcae0e4bee8601170a65b22915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b74d5a07a74da89d6436337c10717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'question', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'input_ids', 'attention_mask', 'prompt_truncated', 'choice_ids'],\n",
       "    num_rows: 12\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = (\n",
    "    dataset\n",
    "    .map(\n",
    "        lambda ex: tokenizer(\n",
    "            ex[\"question\"], padding=\"max_length\", max_length=600, truncation=True, add_special_tokens=True,\n",
    "            # return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(\n",
    "        lambda r: {\"prompt_truncated\": tokenizer.batch_decode(r[\"input_ids\"])},\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(lambda r: {'choice_ids': row_choice_ids(r)})\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.527638Z",
     "start_time": "2023-09-02T11:02:54.527629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.ds/WizardLMWizardCoder_3B_V1.0_imdb_train_12\n"
     ]
    }
   ],
   "source": [
    "# get dataset filename\n",
    "sanitize = lambda s:s.replace('/', '').replace('-', '_') if s is not None else s\n",
    "\n",
    "dataset_name = f\"{sanitize(cfg.model)}_{ds_name}_{split_type}_{N}\"\n",
    "dataset_name\n",
    "f = f\"../.ds/{dataset_name}\"\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.528377Z",
     "start_time": "2023-09-02T11:02:54.528364Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.528958Z",
     "start_time": "2023-09-02T11:02:54.528949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GPTBigCodeForCausalLM(\n",
       "   (transformer): GPTBigCodeModel(\n",
       "     (wte): Embedding(49153, 2816)\n",
       "     (wpe): Embedding(8192, 2816)\n",
       "     (drop): Dropout(p=0.1, inplace=False)\n",
       "     (h): ModuleList(\n",
       "       (0-35): 36 x GPTBigCodeBlock(\n",
       "         (ln_1): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPTBigCodeAttention(\n",
       "           (c_attn): Linear(in_features=2816, out_features=3072, bias=True)\n",
       "           (c_proj): Linear(in_features=2816, out_features=2816, bias=True)\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPTBigCodeMLP(\n",
       "           (c_fc): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "           (c_proj): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "           (act): PytorchGELUTanh()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2816, out_features=49153, bias=False)\n",
       " ),\n",
       " 'tokenizer': GPT2TokenizerFast(name_or_path='WizardLM/WizardCoder-3B-V1.0', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<fim_prefix>', '<fim_middle>', '<fim_suffix>', '<fim_pad>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<empty_output>', '<commit_before>', '<commit_msg>', '<commit_after>', '<reponame>']}, clean_up_tokenization_spaces=True),\n",
       " 'data': Dataset({\n",
       "     features: ['ds_string', 'example_i', 'answer', 'question', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'input_ids', 'attention_mask', 'prompt_truncated', 'choice_ids'],\n",
       "     num_rows: 12\n",
       " }),\n",
       " 'batch_size': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kwargs = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds['choice_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.529566Z",
     "start_time": "2023-09-02T11:02:54.529557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49153, 2816)\n",
       "    (wpe): Embedding(8192, 2816)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2816, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=2816, out_features=2816, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "          (c_proj): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2816, out_features=49153, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_kwargs = dict(cfg=cfg, ds_name=ds_name, split_type=split_type)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.529966Z",
     "start_time": "2023-09-02T11:02:54.529959Z"
    }
   },
   "outputs": [],
   "source": [
    "ds1 = Dataset.from_generator(\n",
    "    generator=batch_hidden_states,\n",
    "    info=DatasetInfo(\n",
    "        description=f\"kwargs={info_kwargs}\",\n",
    "        config_name=f,\n",
    "    ),\n",
    "    gen_kwargs=gen_kwargs,\n",
    ").with_format(\"numpy\")\n",
    "\n",
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Add labels\n",
    "\n",
    "For our probe. Given next_token scores (logits) we take only the subset the corresponds to our negative tokens (e.g. False, no, ...) and positive tokens (e.g. Yes, yes, affirmative, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.530402Z",
     "start_time": "2023-09-02T11:02:54.530395Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets.scores import choice2id, choice2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.530973Z",
     "start_time": "2023-09-02T11:02:54.530965Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_choices(choices: List[str]) -> List[str]:\n",
    "    \"\"\"expand out choices by adding versions that are upper, lower, whitespace, etc\"\"\"\n",
    "    new = []\n",
    "    for c in choices:\n",
    "        new.append(c)\n",
    "        new.append(c.upper())\n",
    "        new.append(c.capitalize())\n",
    "        new.append(c.lower())\n",
    "    return set(new)\n",
    "\n",
    "\n",
    "left_choices = list(r[0] for r in ds1['answer_choices'])+['no', 'false', 'negative', 'wrong']\n",
    "right_choices = list(r[1] for r in ds1['answer_choices'])+['yes', 'true', 'positive', 'right']\n",
    "left_choices, right_choices = expand_choices(left_choices), expand_choices(right_choices)\n",
    "expanded_choices = [left_choices, right_choices]\n",
    "expanded_choice_ids = choice2ids(expanded_choices, tokenizer)\n",
    "expanded_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.531369Z",
     "start_time": "2023-09-02T11:02:54.531361Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets.scores import scores2choice_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.532062Z",
     "start_time": "2023-09-02T11:02:54.532054Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is just based on pairs for that answer...\n",
    "add_txt_ans0 = lambda r: {'txt_ans0': tokenizer.decode(r['scores0'].argmax(-1))}\n",
    "add_txt_ans1 = lambda r: {'txt_ans1': tokenizer.decode(r['scores1'].argmax(-1))}\n",
    "\n",
    "def row_choice_ids(r, tokenizer):\n",
    "    return choice2ids([[c] for c in r['answer_choices']], tokenizer)\n",
    "\n",
    "# Either just use the template choices\n",
    "add_ans = lambda r: scores2choice_probs(r, row_choice_ids(r))\n",
    "\n",
    "# Or all expanded choices\n",
    "add_ans_exp = lambda r: scores2choice_probs(r, expanded_choice_ids, prefix=\"expanded_\")\n",
    "\n",
    "ds3 = (\n",
    "    ds1\n",
    "    .map(add_ans)\n",
    "    .map(add_ans_exp)\n",
    "    .map(add_txt_ans0)\n",
    "    .map(add_txt_ans1)\n",
    ")\n",
    "ds3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.532589Z",
     "start_time": "2023-09-02T11:02:54.532582Z"
    }
   },
   "outputs": [],
   "source": [
    "ds3.save_to_disk(f)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.533085Z",
     "start_time": "2023-09-02T11:02:54.533078Z"
    }
   },
   "outputs": [],
   "source": [
    "ds4 = load_from_disk(f)\n",
    "ds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.533535Z",
     "start_time": "2023-09-02T11:02:54.533528Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# QC, check which answers are most common\n",
    "common_answers = pd.Series(ds4['txt_ans1']).value_counts()\n",
    "display('Remember it should be binary. Found common LLM answers:', common_answers)\n",
    "\n",
    "\n",
    "current_choices = set(list(chain(*ds4['answer_choices'])))\n",
    "unexpected_answers = set(common_answers.head(10).index)-current_choices\n",
    "if len(unexpected_answers):\n",
    "    logger.warning(f'found unexpected answers: {unexpected_answers}. You may want to add them to class2choices')\n",
    "    \n",
    "mean_prob = ds4['choice_probs1'].sum(-1).mean()\n",
    "print('mean_prob', mean_prob)\n",
    "assert ds4['choice_probs1'].sum(-1).mean()>0.4, f\"\"\"\n",
    "Our choices should cover most common answers. But they accounted for a mean probability of {mean_prob:2.2%} (should be >40%). \n",
    "\n",
    "To fix this you might want to improve your prompt or add to your choices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534012Z",
     "start_time": "2023-09-02T11:02:54.534004Z"
    }
   },
   "outputs": [],
   "source": [
    "df = ds2df(ds4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534378Z",
     "start_time": "2023-09-02T11:02:54.534370Z"
    }
   },
   "outputs": [],
   "source": [
    "# QC check accuracy\n",
    "# it should manage to lie some of the time when asked to lie. Many models wont lie unless very explicitly asked to, but we don't want to do that, we want to leave some ambiguity in the prompt\n",
    "\n",
    "d = df.query('instructed_to_lie==True')\n",
    "acc = (d.label_instructed==d.llm_ans).mean()\n",
    "print(f\"when the model tries to lie... we get this acc {acc:2.2f}\")\n",
    "assert acc>0.1, f\"should be acc>0.1 but is acc={acc}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.534845Z",
     "start_time": "2023-09-02T11:02:54.534837Z"
    }
   },
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    return dict(\n",
    "        acc=(df.llm_ans == df.label_instructed).mean(),\n",
    "        n=len(df),\n",
    "    )\n",
    "    \n",
    "def col2statsdf(df, group):\n",
    "    return pd.DataFrame(df.groupby(group).apply(stats).to_dict()).T\n",
    "    \n",
    "    \n",
    "print(\"how well does it do the simple task of telling the truth, for each template\")\n",
    "col2statsdf(df.query('sys_instr_name==\"truth\"'), 'template_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.535378Z",
     "start_time": "2023-09-02T11:02:54.535370Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"how well does it complete the task for each prompt\")\n",
    "# of course getting it to tell the truth is easy, but how effective are the other prompts?\n",
    "col2statsdf(df, 'sys_instr_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC view row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.535946Z",
     "start_time": "2023-09-02T11:02:54.535938Z"
    }
   },
   "outputs": [],
   "source": [
    "# QC by viewing a row\n",
    "r = ds4[0]\n",
    "print(r['prompt_truncated'])\n",
    "print(r['txt_ans1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC: generation\n",
    "\n",
    "Let's a quick generation, so we can QC the output and sanity check that the model can actually do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.536320Z",
     "start_time": "2023-09-02T11:02:54.536313Z"
    }
   },
   "outputs": [],
   "source": [
    "# r = ds[2]\n",
    "# q = r[\"prompt_truncated\"]\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# sequences = pipeline(\n",
    "#     q.lstrip('<|endoftext|>'),\n",
    "#     max_length=100,\n",
    "#     do_sample=False,\n",
    "#     return_full_text=False,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(\"-\" * 80)\n",
    "#     print(q)\n",
    "#     print(\"-\" * 80)\n",
    "#     print(f\"`{seq['generated_text']}`\")\n",
    "#     print(\"-\" * 80)\n",
    "#     print(\"label\", r['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# QC: linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.536825Z",
     "start_time": "2023-09-02T11:02:54.536818Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.537283Z",
     "start_time": "2023-09-02T11:02:54.537276Z"
    }
   },
   "outputs": [],
   "source": [
    "hs = ds4['hs1']-ds4['hs0']\n",
    "X = hs.reshape(hs.shape[0], -1)\n",
    "y = (ds4['ans1'] - ds4['ans0'])>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.537788Z",
     "start_time": "2023-09-02T11:02:54.537780Z"
    }
   },
   "outputs": [],
   "source": [
    "true_switch_sign = ds4['label_true']*2-1\n",
    "# true_switch_sign = ds4['true'][:, 0]*2-1\n",
    "y = ((ds4['ans1'] - ds4['ans0']) * true_switch_sign) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.538282Z",
     "start_time": "2023-09-02T11:02:54.538275Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# # Define X and y\n",
    "# X = dm.hs1-dm.hs2\n",
    "# y = dm.y>0\n",
    "\n",
    "# split\n",
    "n = len(y)\n",
    "max_rows = 1000\n",
    "print('split size', n//2)\n",
    "X_train, X_test = X[:n//2], X[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "X_train = X_train[:max_rows]\n",
    "y_train = y_train[:max_rows]\n",
    "X_test = X_test[:max_rows]\n",
    "y_test = y_test[:max_rows]\n",
    "\n",
    "# scale\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train2 = scaler.transform(X_train)\n",
    "X_test2 = scaler.transform(X_test)\n",
    "print('lr')\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\", penalty=\"l2\", max_iter=380)\n",
    "lr.fit(X_train2, y_train>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T11:02:54.538739Z",
     "start_time": "2023-09-02T11:02:54.538731Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Logistic cls acc: {:2.2%} [TRAIN]\".format(lr.score(X_train2, y_train>0)))\n",
    "print(\"Logistic cls acc: {:2.2%} [TEST]\".format(lr.score(X_test2, y_test>0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
