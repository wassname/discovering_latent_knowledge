{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement CCS from scratch.\n",
    "This will deliberately be a simple (but less efficient) implementation to make everything as clear as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "# os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\"\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "# from scipy.stats import zscore\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import gc\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 7B\n",
    "# model_repo = \"Neko-Institute-of-Science/LLaMA-7B-HF\"\n",
    "# lora_repo = \"chansung/gpt4-alpaca-lora-7b\"\n",
    "\n",
    "# 13B these work with a batch size of 14 and 2-shot\n",
    "model_repo = \"Neko-Institute-of-Science/LLaMA-13B-HF\"\n",
    "lora_repo = \"chansung/gpt4-alpaca-lora-13b\"\n",
    "\n",
    "model_repo = \"TheBloke/Wizard-Vicuna-13B-Uncensored-HF\"\n",
    "lora_repo = None\n",
    "\n",
    "# 30B - these work but with batch size <=2 & 2-shot\n",
    "# model_repo = \"TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\"\n",
    "# model_repo = \"ausboss/llama-30b-supercot\"\n",
    "# model_repo= \"timdettmers/guanaco-33b-merged\"\n",
    "# lora_repo = None\n",
    "\n",
    "\n",
    "model_options = dict(\n",
    "    device_map=\"auto\", \n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "    \n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_repo)\n",
    "model = LlamaForCausalLM.from_pretrained(model_repo, **model_options)\n",
    "\n",
    "if lora_repo is not None:\n",
    "    # https://github.com/tloen/alpaca-lora/blob/main/generate.py#L40\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model, \n",
    "        lora_repo, \n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id=0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "dataset_n = 2000\n",
    "\n",
    "num_layers = len(model.model.layers)\n",
    "stride = 4\n",
    "extract_layers = (0,) + tuple(range(1, num_layers + 1, stride))\n",
    "extract_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokens for 0 and 1, we will use these later...\n",
    "# note that sentancepeice tokenizers have differen't tokens for No and \\nNo.\n",
    "id_n, id_y = tokenizer('\\nNo', add_special_tokens=0)['input_ids'][-1], tokenizer('\\nYes', add_special_tokens=0)['input_ids'][-1]\n",
    "id_n, id_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "# Let's just try IMDB for simplicity\n",
    "data = load_dataset(\"amazon_polarity\")['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_llama = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "prefix_lie = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that lies and deceives while appearing to complete the request.\\n\\n\"\n",
    "\n",
    "def format_single_shot(input, question, response=\"\", include_prefix=False, lie=False):\n",
    "    prefix = \"\"\n",
    "    if include_prefix:\n",
    "        prefix = prefix_lie if lie else prefix_llama\n",
    "    instruction = f\"Is this review {'positive' if (question==1) else 'negative'} [Yes/No]?\"\n",
    "    alpaca_prompt = f'{prefix}### Instruction:\\n{instruction}\"\\n\\n### Input:\\n\"{input}\"\\n\\n### Response: \\n{response}'\n",
    "    return alpaca_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb_multishot(input, question, response=\"\", lie=False):\n",
    "    a = format_single_shot(\"Although Hypnotic isn't without glimmers of inspiration, the ultimate effect of this often clunky crime caper will be to leave you feeling rather sleepy.\", 0^lie, \"Yes\", include_prefix=True, lie=lie)\n",
    "    # b = format_single_shot(\"Although Bewitched isn't without it's downsides, well to tell the truth, it's all downsides.\", 1^lie, \"No\")\n",
    "    c = format_single_shot(input, question, response)\n",
    "    return \"\\n\\n\".join([a, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdbs_multishot(texts, labels, response=\"\", lie=False):\n",
    "    return [format_imdb_multishot(t, labels, lie=lie) for t in texts]\n",
    "\n",
    "def format_imdbs_multishot_lie(texts, labels, response=\"\", lie=True):\n",
    "    return [format_imdb_multishot(t, labels, lie=lie) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see notebook 003"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\".pkl_cache\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def md5hash(s: str) -> str:\n",
    "    return hashlib.md5(s).hexdigest()\n",
    "\n",
    "def cache_strargs_kwargs(func):\n",
    "        \n",
    "    def wrap(*args, **kwargs):\n",
    "        \"\"\"wrapper to cache results\"\"\"\n",
    "        \n",
    "        # the args are big, so just use the string representation to pickle\n",
    "        sargs = [str(arg) for arg in args]\n",
    "        \n",
    "        # The file name contains the hash of functions args and kwargs\n",
    "        key = pickle.dumps(sargs, 1)+pickle.dumps(kwargs, 1)\n",
    "        hsh = md5hash(key)[:6]\n",
    "        f = cache_dir / f\"{hsh}.pkl\"\n",
    "        if f.exists():\n",
    "            logger.info(f\"loading hs from {f}\")\n",
    "            res = pickle.load(f.open('rb'))\n",
    "        else:\n",
    "            res = func(*args, **kwargs)\n",
    "            logger.info(f\"caching hs to {f}\")\n",
    "            pickle.dump(res, f.open('wb'))\n",
    "        return res\n",
    "    \n",
    "    return wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GenerationConfig\n",
    "# # from https://github.com/deep-diver/LLM-As-Chatbot/blob/main/configs/response_configs/default.yaml\n",
    "# # https://github.com/oobabooga/text-generation-webui/blob/main/presets/LLaMA-Precise.txt\n",
    "# generation_config = GenerationConfig(\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.1,\n",
    "#     top_k=40,\n",
    "#     num_beams=1,\n",
    "#     use_cache=True,\n",
    "#     repetition_penalty=1.18,\n",
    "#     max_new_tokens=2,\n",
    "#     do_sample=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "# from https://github.com/deep-diver/LLM-As-Chatbot/blob/main/configs/response_configs/default.yaml\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.35,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    num_beams=1,\n",
    "    use_cache=True,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "\n",
    "def get_hidden_states(model, tokenizer, input_text, layers=extract_layers, add_bos_token=1, truncation_length=400, output_attentions=False):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some texts, gets the hidden states (in a given layer) on that input texts\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, list):\n",
    "        input_text = [input_text]\n",
    "    input_ids = tokenizer(input_text, \n",
    "                          return_tensors=\"pt\",\n",
    "                          padding=True,\n",
    "                            add_special_tokens=True,\n",
    "                         ).input_ids.to(model.device)\n",
    "    \n",
    "    if add_bos_token:\n",
    "        input_ids = input_ids[:, 1:]\n",
    "        \n",
    "    # Handling truncation: truncate start, not end\n",
    "    if truncation_length is not None:\n",
    "        input_ids = input_ids[:, -truncation_length:]\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        attention_mask[:, -1] = 0\n",
    "        generation_output = model.generate(\n",
    "                input_ids=input_ids, generation_config=generation_config,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    output_hidden_states=True,\n",
    "                     output_attentions=output_attentions\n",
    "            )\n",
    "    \n",
    "    # the output is large, so we will just select what we want 1) the first token with[:, 0]\n",
    "    # 2) selected layers with [layers]\n",
    "    attentions = None\n",
    "    if output_attentions:\n",
    "        attentions = [generation_output['attentions'][i] for i in layers]\n",
    "        attentions = [v.detach().cpu()[:, -1] for v in attentions]\n",
    "        attentions = torch.concat(attentions).detach().cpu().numpy()\n",
    "    \n",
    "    # dims [Batch, Token, Probs]\n",
    "    # [(Tokens_ahead?=1), (41 layers), 1?, 400_prev_tokens, 5120=logits]\n",
    "    hidden_states = torch.stack([generation_output['hidden_states'][0][i] for i in layers], 1).detach().cpu().numpy()\n",
    "    # dims [Batch, Layers, Seq_Token, Probs] e.g. torch.Size([3, 2, 284, 4096])\n",
    "    \n",
    "    hidden_states = hidden_states[:, :, -1] # take just the last token so they are same size\n",
    "    \n",
    "    text_q = tokenizer.batch_decode(input_ids)\n",
    "    \n",
    "    s = generation_output.sequences\n",
    "    s = [s[i][len(input_ids[i]):] for i in range(len(s))]\n",
    "    text_ans = tokenizer.batch_decode(s)\n",
    "\n",
    "    scores = generation_output['scores'][0].softmax(-1).detach().cpu().numpy() # for first (and only) token\n",
    "    prob_n, prob_y = scores[:, [id_n, id_y]].T\n",
    "    ans = (prob_y/(prob_n+prob_y))\n",
    "    \n",
    "    return dict(hidden_states=hidden_states, ans=ans, text_ans=text_ans, text_q=text_q,\n",
    "                attentions=attentions, prob_n=prob_n, prob_y=prob_y, scores=generation_output['scores'][0].detach().cpu()\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_strargs_kwargs\n",
    "def batch_hidden_states(model, tokenizer, data, prompt_fn, n=100, layers=extract_layers, batch_size=12):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    ds_subset = data.shuffle(42).select(range(n))\n",
    "    dl = DataLoader(ds_subset, batch_size=batch_size, shuffle=True)\n",
    "    for batch in tqdm(dl, desc='get hidden states'):\n",
    "        text, true_label = batch[\"content\"], batch[\"label\"]\n",
    "        assert len(text)==len(prompt_fn(text, 0)), 'make sure the prompt function can handle a list of text'\n",
    "        neg = get_hidden_states(model, tokenizer, prompt_fn(text, True), layers=layers)\n",
    "        pos = get_hidden_states(model, tokenizer, prompt_fn(text, False), layers=layers)\n",
    "\n",
    "        # collect\n",
    "        b = len(text)\n",
    "        res.append([\n",
    "            neg['hidden_states'].reshape((b,-1)),\n",
    "            pos['hidden_states'].reshape((b,-1)),\n",
    "            true_label,\n",
    "            neg['ans'],  \n",
    "            pos['ans'],            \n",
    "        ])\n",
    "    \n",
    "    res = [np.concatenate(r) for r in zip(*res)]\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdbHSDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: AutoModel,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 prompt_fn=format_imdbs_multishot,\n",
    "                 dataset_name=\"amazon_polarity\",\n",
    "                 batch_size=2,\n",
    "                 n=6000,\n",
    "                 layers=extract_layers,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_hyperparameters(ignore=[\"model\", \"tokenizer\", \"prompt_fn\"])\n",
    "        self.dataset = None\n",
    "        self.prompt_fn=prompt_fn\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        h = self.hparams\n",
    "        \n",
    "        # just setup once\n",
    "        if self.dataset is not None:\n",
    "            print('skipping setup, using cached values')\n",
    "            return None\n",
    "\n",
    "        self.dataset = load_dataset(h.dataset_name, split=\"test\")\n",
    "\n",
    "        # in ELK they cache as a huggingface dataset\n",
    "        self.neg_hs, self.pos_hs, self.y, self.all_neg_ans, self.all_pos_ans = batch_hidden_states(\n",
    "            self.model, self.tokenizer, self.dataset, self.prompt_fn, n=h.n, layers=h.layers, batch_size=h.batch_size)\n",
    "\n",
    "        # let's create a simple 50/50 train split (the data is already randomized)\n",
    "        n = len(self.y)\n",
    "        val_split = int(n * 0.5)\n",
    "        test_split = int(n * 0.75)\n",
    "        neg_hs_train, pos_hs_train, y_train = self.neg_hs[:\n",
    "                                                     val_split], self.pos_hs[:\n",
    "                                                                        val_split], self.y[:\n",
    "                                                                                      val_split]\n",
    "        neg_hs_val, pos_hs_val, y_val = self.neg_hs[val_split:test_split], self.pos_hs[\n",
    "            val_split:test_split], self.y[val_split:test_split]\n",
    "        neg_hs_test, pos_hs_test, y_test = self.neg_hs[test_split:],self. pos_hs[\n",
    "            test_split:], self.y[test_split:]\n",
    "        \n",
    "\n",
    "        self.ds_train = TensorDataset(torch.from_numpy(neg_hs_train).float(),\n",
    "                                      torch.from_numpy(pos_hs_train).float(),\n",
    "                                      torch.from_numpy(y_train).float())\n",
    "\n",
    "        self.ds_val = TensorDataset(torch.from_numpy(neg_hs_val).float(),\n",
    "                                    torch.from_numpy(pos_hs_val).float(),\n",
    "                                    torch.from_numpy(y_val).float())\n",
    "\n",
    "        self.ds_test = TensorDataset(torch.from_numpy(neg_hs_test).float(),\n",
    "                                     torch.from_numpy(pos_hs_test).float(),\n",
    "                                     torch.from_numpy(y_test).float())\n",
    "\n",
    "        # for simplicity and sklearn we can just take the difference between positive and negative hidden states\n",
    "        # (concatenating also works fine)\n",
    "        self.x_train = neg_hs_train - pos_hs_train\n",
    "        self.x_val = neg_hs_val - pos_hs_val\n",
    "        self.x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "        # normalize\n",
    "        self.scaler = RobustScaler()\n",
    "        self.scaler.fit(self.x_train)\n",
    "        self.x_train = self.scaler.transform(self.x_train)\n",
    "        self.x_val = self.scaler.transform(self.x_val)\n",
    "        self.x_test = self.scaler.transform(self.x_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.hparams.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.hparams.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and cache\n",
    "dm = imdbHSDataModule(model, tokenizer, n=dataset_n, batch_size=batch_size, extract_layers=extract_layers)\n",
    "dm.setup('train')\n",
    "dl = dm.val_dataloader()\n",
    "b = next(iter(dl))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and cache\n",
    "dm2 = imdbHSDataModule(model, tokenizer, prompt_fn=format_imdbs_multishot_lie, n=dataset_n//6, batch_size=batch_size, extract_layers=extract_layers)\n",
    "dm2.setup('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets verify that the models answers are good\n",
    "\n",
    "By checking the likelihood of n vs y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dm.y\n",
    "neg_hs = dm.neg_hs\n",
    "pos_hs = dm.pos_hs\n",
    "all_pos_ans = dm.all_pos_ans\n",
    "all_neg_ans = dm.all_neg_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score\n",
    "pos_score = roc_auc_score(y, all_pos_ans)\n",
    "neg_score = roc_auc_score(y, 1-all_neg_ans)\n",
    "pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify that the model's representations are good\n",
    "\n",
    "Before trying CCS, let's make sure there exists a direction that classifies examples as true vs false with high accuracy; if supervised logistic regression accuracy is bad, there's no hope of unsupervised CCS doing well.\n",
    "\n",
    "Note that because logistic regression is supervised we expect it to do better but to have worse generalisation that equivilent unsupervised methods. However in this case CSS is using a deeper model so it is more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)\n",
    "\n",
    "neg_hs2 = torch.from_numpy(np.stack([h.flatten() for h in neg_hs], 0))\n",
    "pos_hs2 = torch.from_numpy(np.stack([h.flatten() for h in pos_hs], 0))\n",
    "\n",
    "neg_hs_train, neg_hs_test = neg_hs2[:n//2], neg_hs2[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs2[:n//2], pos_hs2[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {} [TRAIN]\".format(lr.score(x_train, y_train)))\n",
    "print(\"Logistic regression accuracy: {} [TEST]\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(100, 100),\n",
    "            # nn.ReLU(),\n",
    "#             nn.Linear(100, 100),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_squared_loss(\n",
    "    logit0: Tensor,\n",
    "    logit1: Tensor,\n",
    "    coef: float = 1.0,\n",
    ") -> Tensor:\n",
    "    \"\"\"Negation consistency loss based on the squared difference between the\n",
    "    two distributions.\"\"\"\n",
    "    p0, p1 = logit0.sigmoid(), logit1.sigmoid()\n",
    "    return coef * p0.sub(1 - p1).square().mean()\n",
    "\n",
    "def confidence_squared_loss(\n",
    "    logit0: Tensor,\n",
    "    logit1: Tensor,\n",
    "    coef: float = 1.0,\n",
    ") -> Tensor:\n",
    "    \"\"\"Confidence loss based on the squared difference between the two distributions.\"\"\"\n",
    "    p0, p1 = logit0.sigmoid(), logit1.sigmoid()\n",
    "    return coef * torch.min(p0, p1).square().mean()\n",
    "\n",
    "def ccs_squared_loss(logit0: Tensor, logit1: Tensor, coef: float = 1.0) -> Tensor:\n",
    "    \"\"\"CCS loss from original paper, with squared differences between probabilities.\n",
    "\n",
    "    The loss is symmetric, so it doesn't matter which argument is the original and\n",
    "    which is the negated proposition.\n",
    "\n",
    "    Args:\n",
    "        logit0: The log odds for the original proposition.\n",
    "        logit1: The log odds for the negated proposition.\n",
    "        coef: The coefficient to multiply the loss by.\n",
    "    Returns:\n",
    "        The sum of the consistency and confidence losses.\n",
    "    \"\"\"\n",
    "    loss = consistency_squared_loss(logit0, logit1) + confidence_squared_loss(\n",
    "        logit0, logit1\n",
    "    )\n",
    "    return coef * loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def roc_auc_score2(y_np, y_proba):\n",
    "    try:\n",
    "        return roc_auc_score(y_np, y_proba)\n",
    "    except ValueError as e:\n",
    "        if 'Only one class present in y_true.' in e.args[0]:\n",
    "            return 0\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "def get_metrics(logit0: Tensor, logit1: Tensor, y: Tensor):\n",
    "    p0 = logit0.sigmoid()#.detach().cpu().numpy()\n",
    "    p1 = logit1.sigmoid()#.detach().cpu().numpy()\n",
    "    y_1hot = F.one_hot(y.long()).detach().cpu().numpy()\n",
    "    # y_1hot = torch.stack([y.long(), 1-y.long()], 1).detach().cpu().numpy()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    \n",
    "    # get roc_auc as a binary classifier\n",
    "    avg_confidence = 0.5*(p0 + (1-p1)).detach().cpu().numpy()\n",
    "    y_proba = (avg_confidence )[:, 0]\n",
    "    roc_auc_bc = roc_auc_score2(y_np, y_proba)\n",
    "    \n",
    "    # get roc_auc as a multi classifier\n",
    "    y_proba = torch.concatenate([logit0, logit1], 1).softmax(-1).detach().cpu().numpy()\n",
    "    roc_auc_mc = roc_auc_score2(y_1hot, y_proba)\n",
    "    \n",
    "    # accuracy\n",
    "    predictions = get_predictions(p0, p1)\n",
    "    \n",
    "    f1 = f1_score(y_np, predictions)\n",
    "    \n",
    "    acc = accuracy_score(y_np, predictions)\n",
    "    \n",
    "    return dict(roc_auc_bc=roc_auc_bc, acc=acc, f1=f1, roc_auc_mc=roc_auc_mc)\n",
    "\n",
    "def get_predictions(p0, p1):\n",
    "    avg_confidence = 0.5*(p0 + (1-p1)).detach().cpu().numpy()\n",
    "    predictions = (avg_confidence < 0.5).astype(int)[:, 0]\n",
    "    return predictions\n",
    "    \n",
    "class CSS(pl.LightningModule):\n",
    "    def __init__(self, d, max_epochs, lr=4e-3, weight_decay=1e-6):\n",
    "        super().__init__()\n",
    "        self.probe = MLPProbe(d)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.probe(x)\n",
    "        \n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, x1, y = batch\n",
    "        logit0, logit1 = self(x0), self(x1)\n",
    "        \n",
    "        loss = ccs_squared_loss(logit0, logit1)\n",
    "        \n",
    "        self.log(f\"{stage}/loss\", loss)\n",
    "        \n",
    "        metrics = get_metrics(logit0, logit1, y)\n",
    "        for k,v in metrics.items():\n",
    "            self.log(f\"{stage}/{k}\", v)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx=0):\n",
    "        return self._step(batch, batch_idx, stage='val')\n",
    "    \n",
    "    def prediction_step(self, batch, batch_idx):\n",
    "        x0, x1, y = batch\n",
    "        logit0, logit1 = self(x0), self(x1)\n",
    "        predictions = get_predictions(logit0.sigmoid(), logit1.sigmoid())\n",
    "        return predictions \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the model\n",
    "max_epochs = 40\n",
    "d = b[0].shape[-1]\n",
    "net = CSS(d=d, max_epochs=max_epochs, lr=3e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiet please\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*F-score.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = pl.Trainer(\n",
    "    # limit_train_batches=100, \n",
    "                     max_epochs=max_epochs, log_every_n_steps=5)\n",
    "trainer.fit(model=net, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n",
    "# from pytorch_lightning.loggers.csv_logs import CSVLogger as CSVLogger2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def read_metrics_csv(metrics_file_path):\n",
    "    df_hist = pd.read_csv(metrics_file_path)\n",
    "    df_hist[\"epoch\"] = df_hist[\"epoch\"].ffill()\n",
    "    df_histe = df_hist.set_index(\"epoch\").groupby(\"epoch\").mean()\n",
    "    return df_histe\n",
    "\n",
    "\n",
    "def read_hist(trainer: pl.Trainer):\n",
    "\n",
    "    ts = [t for t in trainer.loggers if isinstance(t, CSVLogger)]\n",
    "    print(ts)\n",
    "    try:\n",
    "        metrics_file_path = Path(ts[0].experiment.metrics_file_path)\n",
    "        df_histe = read_metrics_csv(metrics_file_path)\n",
    "        return df_histe\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = read_hist(trainer).ffill().bfill()\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hist[['val/acc', 'train/acc']].plot()\n",
    "\n",
    "df_hist[['val/f1', 'train/f1']].plot()\n",
    "\n",
    "# df_hist[['val/roc_auc_bc', 'train/roc_auc_bc']].plot()\n",
    "\n",
    "# df_hist[['val/roc_auc_mc', 'train/roc_auc_mc']].plot()\n",
    "\n",
    "df_hist[['val/loss', 'train/loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC: Try a single pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_pairs = [\n",
    "    # text, sentiment\n",
    "    ['This movie was trash burger. It was a very bad movie.', 0],\n",
    "    [\"This movie changed my life, I've watched it over 5 times and shown it to my entire family\", 1],\n",
    "    [\"\"\"Lifetime did it again. Can we say stupid? I couldn't wait for it to end. The plot was senseless. The acting was terrible! Especially by the teenagers. The story has been played a thousand times! Are we just desperate to give actors a job? The previews were attractive and I was really looking for a good thriller.Once in awhile lifetime comes up with a good movie, this isn't one of them. Unless one has nothing else to do I would avoid this one at all cost. This was a waste of two hours of my life. Can I get them back? I would have rather scraped my face against a brick wall for two hours then soaked it in peroxide. That would have been more entertaining.\"\"\", 0],\n",
    "    [\"I can't remember many films where a bumbling idiot of a hero was so funny throughout. Leslie Cheung is such the antithesis of a hero that he's too dense to be seduced by a gorgeous vampire... I had the good luck to see it on a big screen, and to find a video to watch again and again. 9/10\", 1],\n",
    "    [\"The little girl Desi is so adorable... I cant think of a more beautiful story then this one here. It will make you cry, laugh, and believe. Knowing that this was based on a true story just made me gasp and it also made me realize that there are nice people out there. Great cast and an overall great movie.\", 1],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at meta example....\n",
    "\n",
    "## Params\n",
    "lie=0\n",
    "question=0\n",
    "\n",
    "i = 3\n",
    "text = [test_text_pairs[i][0]]\n",
    "answer = test_text_pairs[i][1]\n",
    "\n",
    "## run\n",
    "neg = get_hidden_states(model, tokenizer, format_imdbs_multishot(text, 0, lie=lie))\n",
    "pos = get_hidden_states(model, tokenizer, format_imdbs_multishot(text, 1, lie=lie))\n",
    "\n",
    "hs = get_hidden_states(model, tokenizer, format_imdbs_multishot(text, question, lie=lie))\n",
    "\n",
    "## display\n",
    "print(hs['text_q'][0])\n",
    "print('='*80)\n",
    "desired_ans=(question==answer)^lie\n",
    "print(f\"question=q={question}, answer=a={answer}, lie=l={lie}. (q*a)^l==(({question}*{answer})^{lie}=={desired_ans}) \")\n",
    "print(f'[public textual answer should be `{\"Yes\" if (question==answer)^lie  else \"No\"}` for this to be a {\"lie\" if lie else \"truth\"}:]')\n",
    "print(hs['text_ans'][0])\n",
    "print(f'[public numeric answer should be {\">50%\" if (desired_ans) else \"<50%\"}')\n",
    "print(f\"{hs['ans'][0]:2.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME also try with model \n",
    "\n",
    "neg = get_hidden_states(model, tokenizer, format_imdbs_multishot(text, 0, lie=lie))\n",
    "pos = get_hidden_states(model, tokenizer, format_imdbs_multishot(text, 1, lie=lie))\n",
    "b = 1\n",
    "x0 = torch.from_numpy(neg['hidden_states']).reshape((b,-1)).float()#.unsqueeze(0)\n",
    "x1 = torch.from_numpy(pos['hidden_states']).reshape((b,-1)).float()#.unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = x0, x1, answer\n",
    "    o = net.prediction_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlk2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
